# 帐号 root 密码123456

# **端口号**

hadoop102 [Namenode information](http://hadoop102:9870/dfshealth.html#tab-overview) 

hadoop103 [About the Cluster](http://hadoop103:8088/cluster/cluster) 

# 集群网络配置

配置集群网络环境，要保证**三者一致**:

**主机**：网络-属性-VMNET8-右键-ipv4 

要求：ip与虚拟机网段相同，子ip不同(倒数第二位一样，倒数第一位不一样)

网关与后两者一样

<img src="大数据学习.assets/1628318928419.png" alt="1628318928419" style="zoom:50%;" />

**虚拟机**：编辑-虚拟网络编辑器，vmnet8、nat设置

看网段是否正确，以及nat设置里的网关是否正确

<img src="大数据学习.assets/1628318817380.png" alt="1628318817380" style="zoom:50%;" />

![1628318881062](大数据学习.assets/1628318881062.png)

**linux系统**：sudo vim /etc/sysconfig/network-scripts/ifcfg-ens33 查看是否静态，IP，网关与前两者是否一致

要求IP地址不重复但网段一致，网关一致。

<img src="大数据学习.assets/1628319037526.png" alt="1628319037526" style="zoom:50%;" />

# 1、linux

[尚硅谷大数据技术之Linux](大数据学习.assets/尚硅谷大数据技术之Linux.docx)

## 1.1、常用命令

```python
man 命令 #帮助文档

cd /  #根目录
cd /home/wud #从根目录访问多级子目录，斜杠表示根目录
<home> cd wud #从子目录访问子子目录时不用加斜杠
ls #当前目录所有文件(简要)
ll #当前目录所有文件(详细)
cp 路径/文件名 路径 #复制指定路径的文件到另一个指定路径

shutdown -r now #重启
shutdown -h now #重启

cat 文件1 >> 文件2 #读取文件1的内容追加到文件2的末尾

#改IP
sudo vim /etc/sysconfig/network-scripts/ifcfg-ens33 
sudo vim /etc/hostname #改用户名

#阅读文件
cat 文件名|grep -v INFO #阅读文件并过滤INFO信息
tail -n 200 文件名 #看文件最后200行

mkdir 目录 #新建单个目录
mkdir -p 目录 #新建多级目录
rmdir 目录 #删除目录
touch 文件或者目录/文件 #新建空文件
rm 文件或者目录/文件 #删除指定文件
rm -rf 目录 #递归删除目录下的所有文件
mv 旧文件名 目录 #移动文件
mv 旧文件名 目录/新文件名 #移动加重命名
mv 目录/* 新目录 #移动目录下所有文件

tar -zxvf xiyou.tar.gz -C /opt #解压指定文件到指定目录，z打包同时压缩，x解压，v显示详细信息，f指定压缩后的文件名，-C指定解压到哪儿

sudo chown -R atguigu:atguigu 目录 #递归更改目录的所有者为atguigu
chmod 777 文件名 #为文件添加执行权限
'''
服务命令
'''
systemctl restart network #重启网络服务

chkconfig network --list #检查系统进程运行级别
	#016默认关闭 2345默认开启

systemctl  list-unit-files   	（功能描述：查看所有服务器自启配置）
systemctl  disable 服务名   （功能描述：关掉指定服务的自动启动）
systemctl  enable  服务名  （功能描述：开启指定服务的自动启动）
systemctl  is-enabled 服务名（功能描述：查看服务开机启动状态）

systemctl  start	服务名		（功能描述：开启服务）
systemctle  stop	服务名		（功能描述：关闭服务）
systemctl   restart	 服务名		（功能描述：重新启动服务）
systemctl   status	 服务名		（功能描述：查看服务状态）
systemctl  --type  service		（功能描述：查看正在运行的服务）
```

## 1.2、编辑器vi/vim

![1620658115865](大数据学习.assets/1620658115865.png)

## 1.3、网络设置

1.3.1、虚拟网络编辑器-子网IP-192.168.1.0-点击NAT设置

<img src="大数据学习.assets/1620660209151.png" alt="1620660209151" style="zoom:67%;" />

<img src="大数据学习.assets/1620660235191.png" alt="1620660235191" style="zoom:80%;" />

<img src="大数据学习.assets/1620660321590.png" alt="1620660321590" style="zoom:67%;" />

1.3.2、找到网卡VMnet8 右键 属性 IPV4

<img src="大数据学习.assets/1620660361491.png" alt="1620660361491" style="zoom:67%;" />

![1620660391174](大数据学习.assets/1620660391174.png)

1.3.3、在终端重启网络

systemctl restart network

## 1.4、克隆虚拟机

虚拟机-管理-克隆-完全克隆

克隆完成后打开 点击未列出-用root账户登录-打开终端

输入

vim /etc/sysconfig/network-scripts/ifcfg-env33 

修改IP地址

vim /etc/hostname 

修改用户名

## 1.5、安装包

yum provides 包名 #查询包的安装包名称

yum install -y 包名

# 2、hadoop

![1620654828837](大数据学习.assets/1620654828837.png)

hive数据分析

数据仓库组 写sql 主要用hive

实时组 推荐项目 主要用spark flink

## 2.1、组织架构

![1620883432674](大数据学习.assets/1620883432674.png)

HDFS 分布式存储

MapReduce 分布式计算

Yarn 资源调度

解耦后可以实现计算模块的替换，如2.x版本，可以将mapreduce计算替换为spark

### 2.1.1、HDFS

负责管理硬盘。存储需要存储空间和目录，目录用于记录数据的存储位置

NameNode(nn):目录，主

DataNode(dn)：块数据，从

Secondary NameNode(2nn):每隔一段时间对nn备份，不是nn的热备份

![1620884584551](大数据学习.assets/1620884584551.png)

一主n从

![1620884525640](大数据学习.assets/1620884525640.png)

### 2.1.2、YARN架构

负责分配CPU、内存，统称为资源

**常驻集群：**

从机：NodeManager，决定该节点的资源分配

主机：Resouce Manager，处理client的资源请求，决定处理请求的NM

**被调用集群，被调用生成，平时没有：**

任务临时主管：Appmaster

资源容器：Container

**流程为：**client发出资源请求给RM、RM选择某一NM、此时该NM变成这一请求的App master临时主管，AM进行任务评估以确定所需资源，AM依据评估结果向RM发送资源请求，RM根据请求选择分配资源(多个NM)给client。任务结束后，向RM发送任务完成标识，进行任务验收。

每一个任务对应一个App master，每个App master同时做很多任务，Container根据任务数量将每个节点的资源分为很多份。

![1620884908412](大数据学习.assets/1620884908412.png)

### 2.1.3、MapReduce

计算流程

<img src="大数据学习.assets/1620887614618.png" alt="1620887614618" style="zoom:67%;" />

![1620887604656](大数据学习.assets/1620887604656.png)

2.2、配置本地环境

帐户：atguigu

密码：521u1314

## 2.2、集群配置

### 配置基本环境

1、安装常用软件

```python
sudo yum install -y epel-release

sudo yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop git
```

2、修改用户名和IP地址

```python
sudo vim /etc/sysconfig/network-scripts/ifcfg-ens33 #改IP

改成如下内容
DEVICE=ens33
TYPE=Ethernet
ONBOOT=yes
BOOTPROTO=static
NAME="ens33"
IPADDR=192.168.1.101
PREFIX=24
GATEWAY=192.168.1.2
DNS1=192.168.1.2


sudo vim /etc/hostname #改用户名

改成
hadoop101
```

3、克隆两台虚拟机，分别命名为hadoop102、hadoop103

对于hadoop102，用户名修改为hadoop102，ip地址修改为192.168.1.102

对于hadoop103，用户名修改为hadoop103，ip地址修改为192.168.1.103

### 配置分发脚本

1、在/home/atguigu目录下创建xsync脚本

```python
cd /home/atguigu
vim xsync
```

2、编写如下代码

```python
#1. 判断参数个数
if [ $# -lt 1 ]
then
  echo Not Enough Arguement!
  exit;
fi
#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104
do
  echo ====================  $host  ====================
  #3. 遍历所有目录，挨个发送
  for file in $@
  do
    #4 判断文件是否存在
    if [ -e $file ]
    then
      #5. 获取父目录
      pdir=$(cd -P $(dirname $file); pwd)
      #6. 获取当前文件的名称
      fname=$(basename $file)
      ssh $host "mkdir -p $pdir"
      rsync -av $pdir/$fname $host:$pdir
    else
      echo $file does not exists!
    fi
  done
done
```

3、修改脚本 xsync 具有执行权限

```python
chmod +x xsync
```

4、将脚本移动到/bin中，以便全局调用

sudo mv xsync /bin/

5、测试脚本

sudo xsync /bin/xsync

### 配置免密登录

**原理**

SSH（secure shell）

对称加密

​	需经过密钥发送信息

​	密钥的安全性不足，如拦截一个密钥，则全部拦截

非对称加密

​	密钥成对出现

​	如拦截只能拦截一方	

![1620924205166](大数据学习.assets/1620924205166.png)

免密登录的原理即转为非对称加密，将其中一个密钥Q发送给服务器，P留在本地

**操作**

```python
#1、生成公钥和私钥

ssh-keygen -t rsa  

root帐户的生成位置在 /root/.ssh
私有账户的生成位置在 /home/atguigu/.ssh

生成如下三个文件
```

![1620928413447](大数据学习.assets/1620928413447.png)

```python
#2、生成授权密钥

ssh-copy-id hadoop102 

这里hadoop102是本机用户名

生成第四个文件
```

![1620928456393](大数据学习.assets/1620928456393.png)

```python
#3、分发给集群

xsync /root/.ssh #分发root密钥

xsync /home/atguigu/.ssh #分发私有账户密钥
```

### 配置集群JAVA、Hadoop以及环境变量

#### 1、本机安装

找到“D:\迅雷下载\尚硅谷大数据2020.08\04，Hadoop\资料\上课资料（新）\302_尚硅谷大数据技术之hadoop\2.资料\01_jar包”路径下的两个安装包

打开xshell，进入/opt/software界面，点击xshell上方“新建文件传输”，找到上述两个安装包，点击“向右传输”

输入以下代码安装java

tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/

输入以下代码安装hadoop

tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/

#### 2、配置环境变量

输入以下代码，打开本机环境变量文件

sudo vim /etc/profile.d/my_env.sh

添加以下内容

```python
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin

##HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
```

验证本机环境

java -version

hadoop version

#### 3、配置集群

1、分发软件

xsync /opt/module/

2、分发对应环境

sudo xsync  vim /etc/profile.d/my_env.sh

3、确定职能分配

![1620958535673](大数据学习.assets/1620958535673.png)

102 NN DN NM

103 RM DN NM

104 2NN DN NM

4、配置hadoop

进入 cd /opt/module/hadoop-3.1.3/etc/hadoop/

修改以下5个文件

![1620958906084](大数据学习.assets/1620958906084.png)

#### 4、配置历史服务器和日志聚集

**在102操作**

4.1、历史服务器

cd /opt/module/hadoop-3.1.3/etc/hadoop 进入目录

vi mapred-site.xml  打开文件，添加如下内容

```
<!-- 历史服务器端地址 -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop102:10020</value>
</property>

<!-- 历史服务器web端地址 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop102:19888</value>
</property>
```

4.2、配置日志聚集

cd /opt/module/hadoop-3.1.3/etc/hadoop 进入目录

vim yarn-site.xml  打开文件，添加如下内容

```
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>  
        <name>yarn.log.server.url</name>  
        <value>http://hadoop102:19888/jobhistory/logs</value>  
    </property>
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
	</property>
```

![1620990889892](大数据学习.assets/1620990889892.png)

4.3、分发文件

**在102操作**

输入 xsync /opt/module/hadoop-3.1.3/etc/hadoop

#### 5、集群启动与关闭

**5.1、启动**

在102输入：     

```python
hdfs namenode -format   #HDFS格式化（仅第一次）
start-dfs.sh   #启动HDFS
```

在103输入：

```python
start-yarn.sh   #启动yarn
```

在102输入：

```python
#打开历史服务器
mapred --daemon start historyserver  

#删除已经输入的日志
hdfs dfs -rm -R /user/atguigu/output 
```

群发 ：jps，查看各台机器节点配置

![1620993230006](大数据学习.assets/1620993230006.png)

**5.2、关闭**

在102输入：     stop-dfs.sh   启动HDFS

在103输入：     stop-yarn.sh   启动yarn

在102输入：     mapred --daemon stop historyserver  关闭历史服务器

**5.3、额外功能**

单个节点操作

启动|停止单个节点

hdfs --daemon start|stop datanode

hdfs --daemon start|stop namenode

启动|停止单个节点的NodeManageer

yarn --daemon stop|start nodemanager

启动|停止ResourceManager

yarn --daemon start|stop resourcemanager

#### 6、日志

查看日志

```python
cd $HADOOP_HOME/logs

tail -n 200 hadoop-atguigu-namenode-hadoop102.log #看日志文件最后200行
```

#### 7、启动失败的处理

格式化

```
cd $HADOOP_HOME
rm -rf logs data
```

重新进行“5、集群启动”的操作

#### 8、常用命令

1、看端口netstat

netstat -nltp

![1621000404894](大数据学习.assets/1621000404894.png)

![1621000326567](大数据学习.assets/1621000326567.png)

netstat -nltp|grep  端口号   #看程序端口

netstat -nltp|grep 4380

![1621000464769](大数据学习.assets/1621000464769.png)

2、看系统进程top

3、遍历看所有子目录tree

tree 目录

![1621001040475](大数据学习.assets/1621001040475.png)

4、看剩余内存 free

free -h

![1621001123442](大数据学习.assets/1621001123442.png)

5、df -h 看磁盘分区大小

![1621001181761](大数据学习.assets/1621001181761.png)

6、du 看目录内文件占用情况

du -h 目录

![1621001412578](大数据学习.assets/1621001412578.png)

7、看读写性能iotop

### 配置文件解析

#### core-site.xml

```xml
<configuration>
    <property>
        <!-- 配置HDFS服务端 -->
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop102:8020</value>
    </property>
    <property>
        <!-- 配置根目录 -->
        <name>hadoop.data.dir</name>
        <value>/opt/module/hadoop-3.1.3/data</value>
    </property>
    <property>
        <name>hadoop.proxyuser.atguigu.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.atguigu.groups</name>
        <value>*</value>
    </property>

</configuration>
```

#### hdfs-site.xml

```xml
<configuration>
  <property>
    <!-- 保存FsImage镜像的目录，存放hadoop的名称节点nn里的元数据 -->  
    <name>dfs.namenode.name.dir</name>
    <value>file://${hadoop.data.dir}/name</value>
  </property>
  <property>
    <!-- 存放HDFS文件系统数据文件的目录，作用是存放hadoop的数据节点datanode里的多个数据块 -->
    <name>dfs.datanode.data.dir</name>
    <value>file://${hadoop.data.dir}/data</value>
  </property>
    <property>
    <!-- 2nn节点存储用于合并的临时edit文件的目录 --> 
    <name>dfs.namenode.checkpoint.dir</name>
    <value>file://${hadoop.data.dir}/namesecondary</value>
  </property>
    <!-- DN的重启等待时间，如超过此时间仍没收到心跳，重启DN --> 
    <property>
    <name>dfs.client.datanode-restart.timeout</name>
    <value>30</value>
  </property>
  <!-- 默认端口为9868，2nn的服务器地址和端口号 --> 
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>hadoop104:9868</value>
  </property>
  <!-- 记录允许连接的节点名单 -->  
  <property>
    <name>dfs.hosts</name>
    <value>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts</value>
  </property>
  <!-- 节点黑名单 --> 
  <property>
    	<name>dfs.hosts.exclude</name>
        <value>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts.exclude</value>
  </property>
</configuration>

```

#### mapred-site.xml

```xml
<configuration>
    <!-- 执行Mapreduce的运行框架，可以是local、classic、yarn -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <!-- 历史服务器端地址 -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>hadoop102:10020</value>
    </property>

    <!-- 历史服务器web端地址 -->
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>hadoop102:19888</value>
    </property>
</configuration>

```

#### yarn-site.xml

```xml
<configuration>

<!-- Site specific YARN configuration properties -->
    <!-- 以逗号分隔的服务列表，其中服务名称只能包含A- za - z0 -9_，不能以数字开头 -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- rm的主机名称 -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop103</value>
    </property>
    <!-- 覆盖使用的环境变量 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    <!-- 日志聚合开关 -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <!-- 日志聚合服务器的url -->
    <property>  
        <name>yarn.log.server.url</name>  
        <value>http://hadoop102:19888/jobhistory/logs</value>  
    </property>
    <!-- 在删除日志之前保留的时间，-1表示禁用，别设置的太小 -->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>

</configuration>
```

## 2.3、HDFS

### 1、特点及组织架构

![1621049613835](大数据学习.assets/1621049613835.png)

![1621049640286](大数据学习.assets/1621049640286.png)

![1621049751004](大数据学习.assets/1621049751004.png)

client一个大文件被分为几个数据块，由NN分配存储在DN中(可能是任一DN，或多个DN)，

![1621050019204](大数据学习.assets/1621050019204.png)

### 2、常用shell命令

```python
##Hadoop fs 命令分类
使用方法 hadoop fs -命令

#设置副本数
hadoop fs -setrep 10 hdfs文件名 #为指定文件设置10个副本，但实际副本数量不大于设备数量

#本地-》HDFS 上传
put 本地文件 hdfs目录 #上传文件到指定目录
copyFromLocal #与put类似，增加多线程上传的功能
moveFromLocal #上传后删除本地文件
appendToFile #追加一个文件到已经存在的文件末尾
    hadoop fs -appendToFile - 目标文件 #标准输入添加
    hadoop fs -appendToFile 本地文件 hdfs目标文件
    
#HDFS-》HDFS
cp
mv
chown
chmod
mkdir
du 统计文件夹的大小信息
df 
cat
rm 
    
#HDFS-》本地 下载
get hdfs文件 本地目录 #下载文件到本地目录
copyToLocal #与get完全一样
getmerge #合并下载
	hadoop fs -getmerge /*.txt ./1.txt #把hdfs根目录的所有txt文件内容下载到当前本地上一级目录的1.txt文件内 
```

### 3、客户端

#### 3.1、客户端搭建

1、新建maven项目

2、在pom.xml搭建依赖

```xml
    <dependencies>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.12</version>
        </dependency>
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-slf4j-impl</artifactId>
            <version>2.12.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client-runtime</artifactId>
            <version>3.1.3</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client-api</artifactId>
            <version>3.1.3 </version>
        </dependency>
    </dependencies>
```

3、在src/main/resource下新建log4j2.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="error" strict="true" name="XMLConfig">
    <Appenders>
        <!-- 类型名为Console，名称为必须属性 -->
        <Appender type="Console" name="STDOUT">
            <!-- 布局为PatternLayout的方式，
            输出样式为[INFO] [2018-01-22 17:34:01][org.test.Console]I'm here -->
            <Layout type="PatternLayout"
                    pattern="[%p] [%d{yyyy-MM-dd HH:mm:ss}][%c{10}]%m%n" />
        </Appender>

    </Appenders>

    <Loggers>
        <!-- 可加性为false -->
        <Logger name="test" level="info" additivity="false">
            <AppenderRef ref="STDOUT" />
        </Logger>

        <!-- root loggerConfig设置 -->
        <Root level="info">
            <AppenderRef ref="STDOUT" />
        </Root>
    </Loggers>

</Configuration>

```

4、在src/main/java下新建类 com.atguigu.hdfs.HDFSClient

#### 3.2、客户端命令

```java
原理上是对shell命令的封装
    
    FileSystem fs;

    //搭建文件系统，在test之前运行，用于调试
    @Before
    public void before() throws IOException, InterruptedException {
        //0、进行hadoop配置，即配置虚拟机中的site文件，new Configuration()是site配置文件
        Configuration configuration=new Configuration();
        configuration.set("dfs.replication","2"); //修改副本数
        //configuration.set("dfs.blocksize","67108864"); //修改数据块大小
        //1、获取文件系统
        fs = FileSystem.get(URI.create("hdfs://hadoop102:8020"),
                configuration, "atguigu");
    }

    //关闭文件系统，在test之后运行，用于调试
    @After
    public void after() throws IOException {
        //3、关闭文件系统
        fs.close();
    }

    //上传
    @Test
    public void put() throws IOException {
        //操作
        fs.copyFromLocalFile(new Path("D:\\迅雷下载\\尚硅谷大数据2020.08\\04，Hadoop\\资料\\上课资料（新）\\302_尚硅谷大数据技术之hadoop\\1.笔记\\尚硅谷大数据技术之Hadoop（HDFS).docx"),
                new Path("/1.tar.gz"));
    }

    //下载
    @Test
    public void get() throws IOException, InterruptedException {
        //操作
        fs.copyToLocalFile(new Path("/1.tar.gz"),
                new Path("D:\\"));
    }

    //追加
    @Test
    public void append() throws IOException {
        FSDataOutputStream append = fs.append(new Path("/尚硅谷大数据技术之Hadoop（HDFS).docx"));
        append.write("TASFAFSFA".getBytes());
        IOUtils.closeStream(append);
    }

    //ls 查看文件和文件夹
    @Test
    public void ls() throws IOException {
        FileStatus[] fileStatuses = fs.listStatus(new Path("/"));
        for (FileStatus fileStatus:fileStatuses){
            System.out.println(fileStatus.getPath());
            System.out.println(fileStatus.getOwner());
            System.out.println("==========");

        }
    }

    //lf 查看文件
    @Test
    public void lf() throws IOException {
        RemoteIterator<LocatedFileStatus> statusRemoteIterator = fs.listFiles(new Path("/"), true);
        while (statusRemoteIterator.hasNext()){
            LocatedFileStatus fileStatus = statusRemoteIterator.next();
            System.out.println(fileStatus.getPath());
            System.out.println(fileStatus.getOwner());

            BlockLocation[] blockLocations = fileStatus.getBlockLocations();//文件都分为哪些块
            for (int i = 0; i <blockLocations.length; i++) {
                System.out.println("第"+i+"块");
                String[] hosts = blockLocations[i].getHosts();
                for (String host : hosts) {
                    System.out.println(host+"  ");
                }
                System.out.println();

            }
            System.out.println("===========");
        }

        }

    //mv 移动
    @Test
    public void mv() throws IOException{
        fs.rename(new Path("/1.tar.gz"),
                new Path("/2.tar.gz"));
    }
```

### 4、HDFS传输原理

#### 4.1、写数据流程

![1621093825332](大数据学习.assets/1621093825332.png)

客户端接收文件，根据预设的数据块大小计算文件分为几块，创建分布式文件系统对象df，df与nn搭建连接。

df向nn发送上传请求，nn根据文件大小及副本数分配DN，并返回DN列表。

客户端搭建FS数据流对象，根据DN列表，向网络拓扑结构最近的DN请求构建连接，DN依次向DN列表中后续的DN构建串行连接。如果DN1连接失败，则无法传输；如果DN2\DN3连接失败，NN会为未构建的副本做备份(即生成文件目录，等连接成功时再从DN1拷贝副本到DN2\DN3)。

建立连接后，FS数据流对象向DN1传输数据packet，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答 。 

#### 4.2、读数据流程

![1621099193326](大数据学习.assets/1621099193326.png)

1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。

2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。

3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。

4）客户端以为单位接收，先在本地缓存，然后写入目标文件。

**hadoop 机架感知**

感知机架的拓扑结构，然后进行选择

![1621099270134](大数据学习.assets/1621099270134.png)

**网络拓扑-节点距离计算**

节点距离：两个节点到达最近的共同祖先的距离总和  

![1621099506168](大数据学习.assets/1621099506168.png)

![1621099550723](大数据学习.assets/1621099550723.png)

### 5、NN和2NN

**原理及需求**

一个块的索引在NN内存中占150字节，假设每个块数据按默认值为128MB，则64GB的nn，可以保存最多54TB数据的索引

NN操作(写)时希望用操作日志edits.logs，因为这样能够容纳更多的数据

但NN启动时，如果将操作日志的全部数据进行加载，往往要加载几十个TB，所以这时加载64-256GB的索引变得更有优势(更快)，所以启动时更希望有完整的索引镜像fsimage,fsimage为定期备份

一旦系统发生断电重启，系统根据最近的fsimage还原，同时参照edits.logs日志文件，将未保存的操作还原到fsimage上，即fsimage和edits.logs的合并（合并过程叫做checkpoint）

```
首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。
这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。
但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。
```

**具体工作流程**

![1621213045078](大数据学习.assets/1621213045078.png)

阶段1：

NN启动后先加载编辑日志edits和镜像文件fsimage到内存中，这时如果客户端发送增删改请求，NN会先将增删改操作写到edits中，再对内存中加载的数据进行增删改操作。

阶段2：

如果edits日志中的数据已满(比如到达100W条操作)或者规定时间一到，这时2NN请求执行合并操作Checkpoint，NN收到请求，将edits_inprogress_001文件改名edits_inprogress_002，并拷贝出一个只读文件edits_001，期间的新操作会继续滚动更新到edits_inprogress_002中。随后NN将fsimage文件和只读文件edits_001拷贝到2NN中，2NN将这两个文件合并到2NN的内存中，从而生成新的fsimage，称作fsimage.ckpoint。将fsimage.ckpoint发送回2NN，用以取代2NN之前的fsimage。

fsimage:内存的镜像。由于NN端的内存一般为64GB-256GB，对应的硬盘通常为几TB，因此可以将内存的数据存储到本地硬盘，以进行持久化存储。

edits：编辑日志。通常意义上，NN保存的是元数据，每个数据块的所占元数据约150字节，因此64GB的内存，最多能保存54TB数据对应的元数据。因此为了容纳更多的元数据，将操作存储在编辑日志上。编辑日志往往支持1W-100W条操作

checkpoint：合并。通常是根据编辑日志将操作更新到fsimage上，生成的fsimage相当于是新时间节点的内存的备份。

滚动更新：比如edits支持存储100W条操作，那么当存储已满时，存储新操作会将时间上最早的操作删除掉。类似于队列。

**查看目录**

2NN与NN相比，最主要的区别在于**没有实时更新的edits_inprocess**

因此**不能作为NN的热备份**。

NN： 

cd /opt/module/hadoop-3.1.3/data/name/current

ll

![1621216552688](大数据学习.assets/1621216552688.png)

2NN:

cd /opt/module/hadoop-3.1.3/data/namesecondary/current

ll

![1621216740629](大数据学习.assets/1621216740629.png)



块信息路径

/opt/module/hadoop-3.1.3/data/data/current/BP-618875038-192.168.1.102-1620960299182/current/finalized/subdir0/subdir0

BP是Blockpool，数据块池

https://pan.baidu.com/s/1utq5sLdVne1jKFDqre2iVA 提取码：3x3y

### 6、DN

**工作机制**

![1621404504529](大数据学习.assets/1621404504529.png)

**数据完整性**

如下是DataNode节点保证数据完整性的方法。

（1）当DataNode读取Block的时候，它会计算CheckSum。

（2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。

（3）Client读取其他DataNode上的Block。

（4）DataNode在其文件创建后周期验证CheckSum。

**掉线时限设置**

![1621404616344](大数据学习.assets/1621404616344.png)

需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。  

```
<property>
    <name>dfs.namenode.heartbeat.recheck-interval</name>
    <value>300000</value>
</property>
<property>
    <name>dfs.heartbeat.interval</name>
    <value>3</value>
</property>
```

### 7、黑白名单

打开文件

```python
cd /opt/module/hadoop-3.1.3/etc/hadoop 移到操作目录

vim hdfs-site.xml  #打开配置文件
```

hdfs-site.xml文件配置

```python
黑名单配置
<property>
	<name>dfs.hosts.exclude</name>			
    <value>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts.exclude</value>
</property>

白名单配置
<property>
	<name>dfs.hosts</name>
	<value>/opt/module/hadoop-3.1.3/etc/hadoop/dfs.hosts</value>
</property>
```

编辑黑白名单文件

touch dfs.hosts 建立白名单

touch dfs.hosts.exclude 建立黑名单

如果第一次配置黑白名单，需要重启

```python
102： stop-dfs.sh
	start-dfs.sh
103: stop-yarn.sh
    start-yarn.sh
    
```

配置文件分发

xsync hdfs-site.xml

刷新节点

hdfs dfsadmin -refreshNodes  

yarn rmadmin -refreshNodes  

对于黑名单操作：

在`http://hadoop102:9870/dfshealth.html#tab-datanode`查看dn节点状态，如果节点前面显示为橙色decommissioned，在对应节点输入

​         hdfs --daemon stop datanode  关闭指定节点

### 8、多目录配置

打开文件

```
cd /opt/module/hadoop-3.1.3/etc/hadoop 移到操作目录

vim hdfs-site.xml  #打开配置文件
```

在dfs.datanode.data.dir内，以逗号为间隔输入多个目录

```
<property>
        <name>dfs.datanode.data.dir</name>
<value>file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2</value>
</property>
```

### 9、新增节点

**克隆虚拟机并修改hostname、ip**

```python
右键点击hadoop104，点击管理-克隆，更名为hadoop105-开机

#修改ip

sudo vim /etc/sysconfig/network-scripts/ifcfg-ens33 

#改用户名
sudo vim /etc/hostname 
```

**在主机分发环境、hadoop配置**

rsync -av /opt/module/hadoop-3.1.3/etc/hadoop hadoop105:/opt/module/hadoop-3.1.3/etc/

sudo rsync -av /etc/profile.d/ hadoop105:/etc/profile.d

**在主机将添加新增节点的hostname加入workers文件**

sudo vim /opt/module/hadoop-3.1.3/etc/hadoop/workers

**在新增机器递归删除data和logs目录**

cd /opt/module/hadoop-3.1.3

rm -R  logs data

**刷新环境变量**

source /etc/profile 

**启动新增节点**

hdfs --daemon start datanode  

#### 

## 2.4、MapReduce

### 1、特点及核心思想

**优点：简单** 

**缺点：慢**

![1621515218390](大数据学习.assets/1621515218390.png)

![1621515243428](大数据学习.assets/1621515243428.png)

![1621515262552](大数据学习.assets/1621515262552.png)

**核心思想**

map映射

reduce归约

### 2、Hadoop Writable类型

![1621520261936](大数据学习.assets/1621520261936.png)

Hadoop Writable是java类型的包装类，读用get()，写用set()

### 3、wordcount

#### 3.1、官方程序

```java
package org.apache.hadoop.examples;

import java.io.IOException;
import java.io.PrintStream;
import java.util.StringTokenizer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Reducer.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class WordCount
{
  public static void main(String[] args)
    throws Exception
  {
    Configuration conf = new Configuration();
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
    if (otherArgs.length < 2) {
      System.err.println("Usage: wordcount <in> [<in>...] <out>");
      System.exit(2);
    }
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    for (int i = 0; i < otherArgs.length - 1; i++) {
      FileInputFormat.addInputPath(job, new Path(otherArgs[i]));
    }
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[(otherArgs.length - 1)]));

    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }

  public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable>
  {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Reducer<Text, IntWritable, Text, IntWritable>.Context context)
      throws IOException, InterruptedException
    {
      int sum = 0;
        //将一个单词的value中的1进行累加
      for (IntWritable val : values) {
        sum += val.get(); 
      }
      this.result.set(sum);
      context.write(key, this.result);
    }
  }

  public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>
  {
    private static final IntWritable one = new IntWritable(1);
    private Text word = new Text();

     
    public void map(Object key, Text value, Mapper<Object, Text, Text, IntWritable>.Context context) throws IOException, InterruptedException
    {
        //把一行数据用分隔符拆解为迭代器
      StringTokenizer itr = new StringTokenizer(value.toString());
        //将每个单词设置为word
      while (itr.hasMoreTokens()) {
        this.word.set(itr.nextToken());
          //构造键值对，每个word(即key)，后面加一个value=1
        context.write(this.word, one);
      }
    }
  }
}
```

#### 3.2、官方程序调用

cd /opt/module/hadoop-3.1.3/share/hadoop/mapreduce

```
yarn jar 程序包 调用的主类 输入文件 输出目录
即
yarn jar hadoop-mapreduce-examples-3.1.3.jar wordcount /README.txt /output10
```

#### 3.3、自己编写wordcount

创建包 com.atguigu.mr.wordcount，以下在此包中创建

##### **WcMapper类**

```java
package com.atguigu.mr.wordcount;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;


/*
 * 参数一LongWritable 这一行开头在整个文件中的位置(不是第几行，是字符所在位置)
 * 参数二Text 每一行的数据
 * 参数三 输出单词
 * 参数四  输出单词的出现次数
*/

public class WcMapper extends Mapper<LongWritable,Text,Text,IntWritable>{

    private Text word=new Text();
    private IntWritable one =new IntWritable(1);

    /**
     * 框架将数据拆成一行一行来输出，把数据变成(单词，1)的形式
     * @param key 行号
     * @param value 行内容
     * @param context 任务本身，框架
     **/
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        //super.map(key, value, context);
        //拿到一行数据
        String line=value.toString();
        //拆解为多个单词
        String[] words = line.split(" ");
        //将(单词，1)写回框架
        for (String word : words) {
            this.word.set(word);
            context.write(this.word,this.one);
        }
    }
}
```

##### **WcReducer类**

```java
package com.atguigu.mr.wordcount;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;


/*
 * @description:
 * @param null
 * @return:
 **/
public class WcReducer extends Reducer<Text, IntWritable,Text, IntWritable> {

    private IntWritable result=new IntWritable();
    /**
     * @description: 框架把数据按照单词分好组输入给我们，我们将同一个单词的次数相加
     * map阶段会将单词进行一系列的处理，将单词与对应所有的1为一组输入给reduce
     * 例 (apple,[1,1,1,1,1]),apple为key，[1,1,1,1,1]为Iterable values
     * @param key 单词
     * @param values 这个单词所有的1,类似所有1的集合
     * @param context 任务本身
     **/
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        //super.reduce(key, values, context);
        //求和，计算单词出现多少次
        int sum=0;
        for (IntWritable value : values) {
            sum+=value.get();
        }
        //将求和结果存储到指定类型
        this.result.set(sum);
        //输出
        context.write(key,this.result);
    }
}
```

##### WcDriver类(主类)

```java
package com.atguigu.mr.wordcount;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

/*
 * @description:主方法
 * @param null
 * @return:
 **/
public class WcDriver {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        //1. 建立Job实例
        Job job = Job.getInstance(new Configuration());

        //2. 设置Jar包
        job.setJarByClass(WcDriver.class);

        //3. 设置Mapper和Reducer
        job.setMapperClass(WcMapper.class);
        job.setReducerClass(WcReducer.class);

        //4. 设置Map和Reduce输出类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        //5. 设置输入输出文件
        FileInputFormat.setInputPaths(job,new Path(args[0]));
        FileOutputFormat.setOutputPath(job,new Path(args[1]));

        //6. 提交Job
        boolean b = job.waitForCompletion(true);

        System.exit(b?0:1);
    }
}
```

#### 3.4、自己程序调用

点击右侧Maven.点击package,会在target目录下生成一个.jar文件

拖动生成的.jar文件到xshellc的hadoop102窗口，显示rz -E,即上传成功

ll 检查一下

cd /opt/module/hadoop-3.1.3

```shell
yarn jar 包名 具体启动类 输入文件 输出路径

yarn jar mapreduce200105-1.0-SNAPSHOT.jar com.atguigu.mr.wordcount.WcDriver /README.txt /output7
```

### 4、序列化

![1622963619607](大数据学习.assets/1622963619607.png)

把程序对象转换为字节序列，叫做序列化

把字节序列存储在磁盘上，叫做持久化

把字节序列通过网络传输给远程计算机，叫做网络传输

注意：基于框架写bean时，一定留一个无参构造器，用于反射

### 5、MapReduce数据流

![1624848678761](大数据学习.assets/1624848678761.png)

包含三个阶段，两个Task，其中：

MapTask包含InputFormat、Mapper、Shuffle的前半部分

ReduceTask包含OutputFormat、Reducer、Shuffle的后半部分

![1624848944028](大数据学习.assets/1624848944028.png)

数据分几份就启动几个MapTask

#### 5.1、InputFormat

![1624849303267](大数据学习.assets/1624849303267.png)

shuffle非常需要网络带宽

但此刻数据分块，如果均分为100M，由于DN1中存储的数据为128M，因此DN1中128-100之外剩余的需要网络传输到DN2中，以此类推。差值越大，占用的网络带宽越大

所以一般将切片大小=DN中的文件块大小，即物理切分和逻辑切分对应，尽量避免网络传输

任务切片在yarn客户端完成，即JAVA的Driver类

##### 5.1.1、任务提交流程

```java
流程总结
1、验证job状态，job状态包含定义(DEFINE)和运行中(RUNNING)两种，如果状态为定义，则执行提交submit
2、执行ensureState，再次确认job状态，如果不是“DEFINE”，则执行异常处理
3、执行setUseNewAPI，由于存在版本更新，这里将所有老版本api转为新版本api
4、执行connect：
如果集群没有创建，新建集群new Cluster(getConfiguration());。此时Cluster的单参构造运行，创建了默认jobTrackAddr，同时调用了Cluster的双参构造，执行initialize函数：
	读取服务器中mapreduce.xml文件配置的mapreduce.framework.name的值。先看是否为yarn，如果是则创建yarn集群；再看是否为local，如果是则创建本机集群。
5、创建job提交器JobSubmitter submitter，获取集群的文件系统信息和客户端信息
6、提交job给集群，执行submitter.submitJobInternal：
//检查Driver类中job设置的输出配置
checkSpecs(job);
//创建给集群提交数据的stag路径
Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);
//从客户端申请任务ID，并赋给job
    JobID jobId = submitClient.getNewJobID();
    job.setJobID(jobId);
//根据上面申请的路径及id，创建临时存储目录
Path submitJobDir = new Path(jobStagingArea, jobId.toString());
//拷贝任务的jar包到临时存储目录
copyAndConfigureFiles(job, submitJobDir);
	//创建任务提交器，执行jar包提交
	rUploader.uploadResources(job, jobSubmitDir);
//执行切片，将切片信息保存到临时存储目录
int maps = writeSplits(job, submitJobDir);
//保存XML文件

```

文件输入后，inputformat将其逻辑切分为几块，同时每块对应一个MapTask，此时inputformat创建的RecordReader将每个数据切片转化成K\V值列表，并传给Mapper

MapTask.run 执行Map阶段，在MapTask中调用自己写的Mapper.map方法

##### 5.1.2、切片过程

JobSubmitter类----writeSplits方法----writeNewSplits方法----getSplits方法，方法来自于InputFormat类，返回值为InputSplit类

```java
  private <T extends InputSplit>
  int writeNewSplits(JobContext job, Path jobSubmitDir){
    Configuration conf = job.getConfiguration();
      //应用反射生成input对象
    InputFormat<?, ?> input =
      ReflectionUtils.newInstance(job.getInputFormatClass(), conf);
      //获取切片
    List<InputSplit> splits = input.getSplits(job);
    T[] array = (T[]) splits.toArray(new InputSplit[splits.size()]);

    // sort the splits into order based on size, so that the biggest
    // go first
    Arrays.sort(array, new SplitComparator());
    JobSplitWriter.createSplitFiles(jobSubmitDir, conf, 
        jobSubmitDir.getFileSystem(conf), array);
    return array.length;
  }
```

##### 5.1.3、InputFormat抽象父类及实现类

![1625017547567](大数据学习.assets/1625017547567.png)

父类

```java
public abstract class InputFormat<K, V>
    
con
    
    两个抽象方法,分别对应两个抽象父类：InputSplit、RecordReader
//对输入文件进行逻辑切分，每个InputSplit切片指派给一个独立的Mapper去处理
//注意逻辑切分不同于物理切分，切分可以是文件路径、起始、终止；也可以是元组 
List<InputSplit> getSplits(JobContext context)   

//如何读取切片的数据，并将逻辑切分转化为k\v对，传给Mapper
RecordReader<K,V> createRecordReader(InputSplit split,TaskAttemptContext context)
```

子类

FileInputFormat类--getSplits函数

```java
public abstract class FileInputFormat<K, V>
    
public List<InputSplit> getSplits(JobContext job){
    //同步单线程代码块
    StopWatch sw = new StopWatch().start();
    //getFormatMinSplitSize返回1
    //如果没设置最小值，返回1，否则返回配置中的自设值
    long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
	//如果没设置最大值，返回long类型最大值，否则返回自设值
    long maxSize = getMaxSplitSize(job);

    // generate splits 创建切分列表(容器)
    List<InputSplit> splits = new ArrayList<InputSplit>();
    List<FileStatus> files = listStatus(job);

    boolean ignoreDirs = !getInputDirRecursive(job)
      && job.getConfiguration().getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, false);
    for (FileStatus file: files) {
//如果传入的是个目录，则跳过
      if (ignoreDirs && file.isDirectory()) {
        continue;
      }
      Path path = file.getPath();
//获取文件大小
      long length = file.getLen();
      if (length != 0) {
        BlockLocation[] blkLocations;
        if (file instanceof LocatedFileStatus) {
          blkLocations = ((LocatedFileStatus) file).getBlockLocations();
        } else {
          FileSystem fs = path.getFileSystem(job.getConfiguration());
          blkLocations = fs.getFileBlockLocations(file, 0, length);
        }
        if (isSplitable(job, path)) {
//获取文件的分块尺寸列表(物理)
          long blockSize = file.getBlockSize();
//根据上面定义的最小尺寸、最大尺寸，物理分块尺寸计算逻辑分块尺寸
          long splitSize = computeSplitSize(blockSize, minSize, maxSize);
//SPLIT_SLOP=1.1，对文件逐次切分，
          long bytesRemaining = length;
          while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) {
            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
//执行切分，并放入切分列表
            splits.add(makeSplit(path, length-bytesRemaining, splitSize,               blkLocations[blkIndex].getHosts(),                     blkLocations[blkIndex].getCachedHosts()));
//切分后计算剩余大小
            bytesRemaining -= splitSize;
          }
//如果最后剩余的尺寸<1.1*切片尺寸，且不为0
          if (bytesRemaining != 0) {
            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
//将剩余大小分为一篇
            splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,
blkLocations[blkIndex].getHosts(),
blkLocations[blkIndex].getCachedHosts()));
          }
        } 
    return splits;
  }
```

##### 5.1.4、InputSplit类

![1625017589404](大数据学习.assets/1625017589404.png)

子类FileSplit

FileInputFormat类--getSplits函数--makeSplit函数--FileSplit类有参构造函数

```java
//file 文件名
//start 文件中第一个处理的字节位置
//length 处理文件的字节数
//hosts 物理存储块的客户端列表
//inMemoryHosts 逻辑分块的客户端列表

public FileSplit(Path file, long start, long length, String[] hosts,
     String[] inMemoryHosts){
   this(file, start, length, hosts);
   hostInfos = new SplitLocationInfo[hosts.length];
   for (int i = 0; i < hosts.length; i++) {
     // because N will be tiny, scanning is probably faster than a HashSet
     boolean inMemory = false;
       //逐个判断每个逻辑分块的地址与物理存储分块是否一致
     for (String inMemoryHost : inMemoryHosts) {
         //如果物理分块与逻辑分块一致
       if (inMemoryHost.equals(hosts[i])) {
         //分块定位信息为确定
         inMemory = true;
         break;
       }
     }
       //将逻辑分块与物理分块的一致性信息返回
     hostInfos[i] = new SplitLocationInfo(hosts[i], inMemory);
   }
 }
```

##### 5.1.5、RecordReader

![1625017505992](大数据学习.assets/1625017505992.png)

```java
package com.atguigu.mr.inputformat;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;

import java.io.FileInputStream;
import java.io.IOException;

/**
 * @ClassName MyRecordReader
 * @Description
 * @Author Wud
 * @Date 2021/6/29 22:15
 * @Version 1.0
 **/
public class MyRecordReader extends RecordReader<Text, BytesWritable> {

    boolean isprocess=false; //默认false 表示没读

    private Text key=new Text();
    private BytesWritable value=new BytesWritable();

    private FSDataInputStream open;
    private FileSystem fileSystem;
    private FileSplit fs;
    /**
     * @description:初始化时仅执行一次，负责开流
     * @param split 切片的内容
     * @param context 任务请求信息
     **/
    @Override
    public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {
        //将split转换为子类

        fs = (FileSplit) split;
        //创建文件系统
        fileSystem = FileSystem.get(context.getConfiguration());
        //打开文件，用流读取文件
        open = fileSystem.open(fs.getPath());

    }

    //获取下一个切片信息，返回布尔值，读到返回true，否则返回False
    @Override
    public boolean nextKeyValue() throws IOException, InterruptedException {
        if (!isprocess){
            //读取文件
            byte[] bytes = new byte[(int) fs.getLength()];
            open.read(bytes);


            //填充key和value
            key.set(fs.getPath().toString()); //key是文件地址
            value.set(bytes,0,bytes.length);

            //标记读完
            isprocess=true;
            return true;
        }else {
            return false;
        }

    }

    //获取确切的key
    @Override
    public Text getCurrentKey() throws IOException, InterruptedException {
        return key;
    }

    //获取确切的value
    @Override
    public BytesWritable getCurrentValue() throws IOException, InterruptedException {
        return value;
    }

    //获取完成状态，完成为1，否则为0
    @Override
    public float getProgress() throws IOException, InterruptedException {

        return isprocess?1:0;
    }

    /**
     * @description:结束时执行一次，关流
     **/
    @Override
    public void close() throws IOException {

    }

}
```



## 2.5、Shuffle

### 1、大体框架

配置staticuser  core配置文件 解决网页不能创建目录的问题 默认用户为dr.who，设置为atguigu

Map阶段实际执行的是MapTask.run(),该方法过程中会调用我们写的Mapper.map()

将无序的kv对，转换为有序的KV对，因此判断紧邻的kv中key是否相等，相等则分组

collect阶段：分块内部排序，用快速排序。再将多个有序块归并为一个有序块。

<img src="大数据学习.assets/1625138697771.png" alt="1625138697771" style="zoom:50%;" />

![1626934341598](大数据学习.assets/1626934341598.png)

![1626934493308](大数据学习.assets/1626934493308.png)

1、Maptask阶段map不断向outputCollector里面写数据，outputCollector是一个环形缓冲区，每当存储到达100m时，对缓冲区内部进行分区，随后对分区单独进行快速排序，将排序后的文件溢写为.spill文件块。最后对所有.spill文件块进行归并排序。

2、在read阶段，每个文件分片spilts都对应一个maptask，每个maptask最后都会合并为一个文件

3、reduceTask阶段，指定个数的Reducetask再对每个Maptask发来的文件进行归并排序，最后将文件成组发送给Reducer

### 2、分区概念

Reducetask个数 `job.setnumsReduceTask(个数)`

![1626935894234](大数据学习.assets/1626935894234.png)

上图：outputCollector是一个环形缓冲区，每当存储到达100m时，对缓冲区内部进行分区，随后对分区单独进行快速排序，将排序后的文件溢写为.spill文件块。注意每个文件块内每个分区内部有序，随后的归并排序是对每个分区单独归并，结束后仍然是按分区个数输出，每个分区内部有序。所有分区是一整个文件，用索引控制分区。

下图：多个map输出了多个分区有序的文件，每个reduce只取所属分区的文件

### 3、排序

## 2.6、Outputformat

```java
    public RecordWriter<K, V> getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {
        Configuration conf = job.getConfiguration();
        boolean isCompressed = getCompressOutput(job); //任务压缩
        String keyValueSeparator = conf.get(SEPARATOR, "\t");
        CompressionCodec codec = null;
        String extension = "";
        if (isCompressed) {
            Class<? extends CompressionCodec> codecClass = getOutputCompressorClass(job, GzipCodec.class);
            codec = (CompressionCodec)ReflectionUtils.newInstance(codecClass, conf);
            extension = codec.getDefaultExtension();
        }

        Path file = this.getDefaultWorkFile(job, extension);
        FileSystem fs = file.getFileSystem(conf);
        FSDataOutputStream fileOut = fs.create(file, false);
        return isCompressed ? new TextOutputFormat.LineRecordWriter(new DataOutputStream(codec.createOutputStream(fileOut)), keyValueSeparator) : new TextOutputFormat.LineRecordWriter(fileOut, keyValueSeparator);
    }
```



## 2.7、configuration类常用

```java
configuration.set("fs.defaultFS", "hdfs://hadoop102:8020");
configuration.set("mapreduce.framework.name","yarn");
configuration.set("mapreduce.app-submission.cross-platform","true");
configuration.set("yarn.resourcemanager.hostname","hadoop103");  
```

# 3、Zookeeper

## 3.1、安装

### 1、单机配置

**(1)上传安装包**

将安装包`zookeeper-3.5.7.tar.gz  `上传到 /opt/software     

**(2)解压缩**

tar -zxvf zookeeper-3.5.7.tar.gz -C /opt/module/

**(3)添加本地存储目录**

cd /opt/module/zookeeper-3.5.7/  

mkdir -p zkData   这里的-p是创建多级目录

**(4)修改指向本地存储路径**

cd /opt/module/zookeeper-3.5.7/conf  

mv zoo_sample.cfg zoo.cfg   配置文件改名

vim zoo.cfg  编辑配置文件

​	把其中一行dataDir改为：

​		dataDir=/opt/module/zookeeper-3.5.7/zkData

​	添加以下内容

```
#######################cluster##########################
server.2=hadoop102:2888:3888
server.3=hadoop103:2888:3888
server.4=hadoop104:2888:3888
```

**(5)创建myid文件，作为本机唯一标识**

cd /opt/module/zookeeper-3.5.7/zkData  

touch myid

**(6)创建环境变量**

sudo vim /etc/profile.d/my_env.sh 

```
#ZOOKEEPER_HOME
export ZOOKEEPER_HOME=/opt/module/zookeeper-3.5.7
export PATH=$PATH:$ZOOKEEPER_HOME/bin
```

### 2、分发配置

**(1)编写分发脚本(如已有则略过)**

cd /home/atguigu

vim xsync

```
#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]
then
  echo Not Enough Arguement!
  exit;
fi
#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104
do
  echo ====================  $host  ====================
  #3. 遍历所有目录，挨个发送
  for file in $@
  do
    #4 判断文件是否存在
    if [ -e $file ]
    then
      #5. 获取父目录
      pdir=$(cd -P $(dirname $file); pwd)
      #6. 获取当前文件的名称
      fname=$(basename $file)
      ssh $host "mkdir -p $pdir"
      rsync -av $pdir/$fname $host:$pdir
    else
      echo $file does not exists!
    fi
  done
done
```

chmod +x xsync          修改脚本 xsync 具有执行权限  

sudo mv xsync /bin/         将脚本移动到/bin中，以便全局调用  

sudo xsync /bin/xsync           测试脚本  

(2)分发zookeeper软件

cd /opt/module

xsync zookeeper-3.5.7/   分发到脚本定义好的各个服务器

(3)分发环境变量

sudo xsync /etc/profile.d/my_env.sh 

(4)在102节点

cd /opt/module/zookeeper-3.5.7/zkData

vim myid 编辑myid文件，就写一个数字2

```
2
```

(5)在103节点

cd /opt/module/zookeeper-3.5.7/zkData

vim myid 编辑myid文件，就写一个数字3

```
3
```

(6)在104节点

cd /opt/module/zookeeper-3.5.7/zkData

vim myid 编辑myid文件，就写一个数字4

```
4
```

## 3.2、控制命令

### 1、Shell命令

#### 1.1、管理指令

```
cd /opt/module/zookeeper-3.5.7/
注意以下的bin已经在环境变量中定义

启动zookeeper，每个节点需要单独启动
bin/zkServer.sh start 

关闭zookeeper，每个节点需要单独关闭
bin/zkServer.sh stop

查看运行状态
bin/zkServer.s status

启动客户端
bin/zkCli.sh
```

#### 1.2、常用指令

```
注意：所有监听一次有效

ls -W -s path 查看znode子目录
	-w 查看子目录并监听子节点变化 
	-s 查看子目录并附加次级信息
create -s -e path value 在指定路径创建一个子目录，并赋值
	-s 包含序列的节点，在名字后面加一串数字
	-e 创建临时节点，该node重启后消失
get -s -w path 查看子目录的值
	-w 查看值并监听节点的值变化
	-s 查看值及附加信息
set path value 对子目录重新赋值
stat -w path 直接查看附加信息
delete path 删除子目录
rmr path 递归删除子目录
```

<img src="大数据学习.assets/1627199339281.png" alt="1627199339281" style="zoom:150%;" />

### 2、客户端API

```java
public class Client {
    private static String connectString="hadoop102:2181,hadoop103:2181,hadoop104:2181";
    private static int sessionTimeout=2000;
    private ZooKeeper zkClient=null;

    @Before
    public void init() throws IOException {
        zkClient=new ZooKeeper(connectString, sessionTimeout, new Watcher() {
            @Override
            public void process(WatchedEvent event) {
                System.out.println(event.getType() + "--"+event.getPath());
                try {
                    zkClient.getChildren("/",true);
                } catch (KeeperException | InterruptedException e) {
                    e.printStackTrace();
                }
            }
        });
    }

    // create
    @Test
    public void create() throws KeeperException, InterruptedException {
        // 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型
        String nodeCreatedInfo = zkClient.create("/atguigu", "jinlian".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT_SEQUENTIAL);
        System.out.println(nodeCreatedInfo);
    }

    // ls
    @Test
    public void ls() throws InterruptedException, KeeperException {
        List<String> children=zkClient.getChildren("/",true);

        for (String child : children) {
            System.out.println(child);
        }
        Thread.sleep(Long.MAX_VALUE);
    }

    //stat
    @Test
    public void exist() throws KeeperException, InterruptedException {
        Stat stat = zkClient.exists("/test", false);
        System.out.println(stat==null?"not exist":"exist");
        System.out.println(stat);
    }

    //get
    @Test
    public void get() throws KeeperException, InterruptedException, IOException {
        Stat stat = new Stat();
        byte[] zkClientData = zkClient.getData("/test", false, stat);
        System.out.write(zkClientData);
        System.out.println(stat.getCtime());
    }

    //set version是修改次数，乐观锁
    @Test
    public void set() throws KeeperException, InterruptedException {
        String node="/test";
        Stat stat1 = zkClient.exists(node, false);
        if (stat1==null){
            zkClient.setData(node, "asd".getBytes(), stat1.getVersion());
        }else {
            System.out.println("wrong");
        }
    }

    @Test
    public void delete() throws KeeperException, InterruptedException {
        String node="/test";
        Stat stat1 = zkClient.exists(node, false);
        zkClient.delete(node,stat1.getVersion());
    }
}
```

## 3.3、内部原理

### 1、框架

每个zoonode 分为路径path和值value ，node注册完成后，可以监听某个path的增减，也可以监听某个path的value的变化

![1627228465246](大数据学习.assets/1627228465246.png)

### 2、节点类型

分为四种，主要是根据 create -s -e path value

其中-s 为是否序列化 -e为是否为临时目录

用于节点创建阶段的属性设置

![1627228533693](大数据学习.assets/1627228533693.png)

### 3、stat结构体

在ls -s path 和stat path两个命令中，返回的都是stat信息，stat是节点的补充信息

<img src="大数据学习.assets/1627228742914.png" alt="1627228742914" style="zoom:80%;" />

```
（1）czxid-创建节点的事务zxid
每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。
事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。
（2）ctime - znode被创建的毫秒数(从1970年开始)
（3）mzxid - znode最后更新的事务zxid
（4）mtime - znode最后修改的毫秒数(从1970年开始)
（5）pZxid-znode最后更新的子节点zxid
（6）cversion - znode子节点变化号，znode子节点修改次数
（7）dataversion - znode数据变化号
（8）aclVersion - znode访问控制列表的变化号
（9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。
（10）dataLength- znode的数据长度
（11）numChildren - znode子节点数量
```

### 4、监听

![1627228783589](大数据学习.assets/1627228783589.png)

### 5、选举机制

半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。

![1627228823701](大数据学习.assets/1627228823701.png)

![1627228843191](大数据学习.assets/1627228843191.png)

### 6、写数据流程

![1627228879730](大数据学习.assets/1627228879730.png)

# 4、hive

## 4.0、shell命令

```shell
hive -f sql文件
hive -e "sql指令"
1、hive/bin/hiveservices.sh start
2、start-dfs.sh
```

## 4.1、安装

### beeline

hive --service beeline -u jdbc:hive2://hadoop102:10000 -n atguigu

- !connect url –连接不同的Hive2服务器
- !exit –退出shell
- !help –显示全部命令列表
- !verbose –显示查询追加的明细

## 4.2、操作指令

### 1、数据库

#### **创建数据库**

```mysql
#这里location的地址为HDFS绝对路径地址，与库名一一映射，斜杠/表示根路径，后面为该库名的物理文件夹的名称，如果想自己定义路径，则/后面不能为空，如不写location则存储到默认路径下(在hive-site.xml中定义)

create database 库名；
comment "bula bula"
location '/test.db' 
[with dbproperties]
```

#### **查看数据库**

show databases；

#### **选择数据库**

use 库名

#### **查看数据库信息**

desc database [extended] 库名；

#### **删除数据库**

drop database 库名； #删空库

drop database 库名 cascade;   #无视删库

#### **修改数据库**

只能修改其中的键值对(几乎不用)

alter database db_hive set dbproperties('createtime'='20170830');  

### 2、数据表

#### **创建数据表**

```sql
--案例
create table test(
name string comment "Name",
friends array<string>,
children map<string,int>,
address struct<street:string,city:string>
)

row format 
delimited fields terminated by ','
collection items terminated by '_'
map keys terminated by ':'
lines terminated by '\n';

--用查询结果建表
create table stu_result as select * from stu_par where id=1001;
```

```sql
--完整版
EXTERNAL 外部表(
列名 数据类型 [comment 注释内容]
COMMENT 注释
PARTITIONED BY 分区表
ROW FORMAT 列映射方式，内容如何映射为表格，重点。
STORED 存储格式
LOCATION 表在hdfs的存储路径
TBLPROPERTIES  表属性
AS 对搜索结果重命名)
delimited fields表示划分属性

CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name 
[(col_name data_type [COMMENT col_comment], ...)] 
[COMMENT table_comment] 
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] 
[CLUSTERED BY (col_name, col_name, ...) 
[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] 
[ROW FORMAT row_format] 
[STORED AS file_format] 
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
[AS select_statement]
```

#### **查看数据表清单**

show tables;

#### **查看数据表信息**

desc 表名;  --列信息

desc formatted 表名;  --详细信息

#### **修改**

```mysql
--修改一列
alter table test2 change id id string;
--添加一列或多列
alter table test2 add columns(class string comment "class NO.");
--替换多列信息
alter table test2 replace columns(id double comment "ID" , name string);
```

#### **删表**

```mysql
drop table test2;
```

### 3、分区表

#### **一级分区表**

```mysql
--建立一张分区表
create table stu_par
(id int, name string)
partitioned by (class string)
row format delimited fields terminated by '\t';
```

```mysql
--向表中插入数据
load data local inpath '/opt/module/datas/student.txt' into table stu_par
partition (class='01');
load data local inpath '/opt/module/datas/student.txt' into table stu_par
partition (class='02');
load data local inpath '/opt/module/datas/student.txt' into table stu_par
partition (class='03');
```

```mysql
--查表时，选择分区，可以减小数据扫描量
select * from stu_par where class="01";
select * from stu_par where id=1001;
```

```mysql
--查询分区表的分区
show partitions stu_par;
```

```mysql
--如果提前准备数据，但是没有元数据，修复方式
--1. 添加分区
alter table stu_par add partition(class="03");
--2. 直接修复
msck repair table stu_par;
--3. 上传时候直接带分区
load data local inpath '/opt/module/datas/student.txt' into table stu_par
partition (class='03');
```

#### **二级分区表**

```mysql
--建立二级分区表
create table stu_par2
(id int, name string)
partitioned by (grade string, class string)
row format delimited fields terminated by '\t';
```

```mysql
--插入数据，指定到二级分区
load data local inpath '/opt/module/datas/student.txt' into table stu_par2
partition (grade='01', class='03');
```

#### **分区的增删改查**

```mysql
--增加分区
alter table stu_par add partition(class="05");
--一次增加多个分区
alter table stu_par add partition(class="06") partition(class="07");
--删除分区
alter table stu_par drop partition(class="05");
--一次删除多个分区
alter table stu_par drop partition(class="06"), partition(class="07");
```

### 4、DML

#### 1. 数据导入

```mysql
--从本地磁盘或者DHFS导入数据
load data [local] inpath '/opt/module/datas/student.txt' [overwrite] into table student [partition (partcol1=val1,…)];

--例子
load data local inpath '/opt/module/datas/student.txt' overwrite into table student;

--先在hdfs://hadoop102:8020/xxx文件夹上传一份student.txt
--HDFS的导入是移动，而本地导入是复制
load data inpath '/xxx/student.txt' overwrite into table student;
```

```mysql
--Insert导入
insert into table student select id, name from stu_par where class="01";
```

```
--建表时候用as select
```

```mysql
--建表时候通过location加载
--先在hdfs://hadoop102:8020/xxx文件夹上传一份student.txt
create external table student2
(id int, name string)
row format delimited fields terminated by '\t'
location '/xxx';
```

#### 2. 数据导出

```mysql
--Insert导出
insert overwrite local directory '/opt/module/datas/export/student'
select * from student;

--带格式导出
insert overwrite local directory '/opt/module/datas/export/student1'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
select * from student;
```

```bash
#bash命令行导出
hive -e "select * from default.student;" > /opt/module/datas/export/test.txt
```

```mysql
--整张表export到HDFS
export table student to '/export/student';

--从导出结果导入到Hive
import table student3 from '/export/student';
```

#### 3. 数据删除

```mysql
--只删表数据，不删表本身
truncate table student;
```

## 4.3 常用函数

date_format(时间,'yyyy-MM-dd') 时间格式化

 



# 5、Spark

## 5.1、基础概念

### 集群角色

#### 1、master和worker

master 接受分配资源

worker 执行程序

在standalone模式下特有

![1629876258665](大数据学习.assets/1629876258665.png)

#### 2、driver和executor

在standalone和yarn模式下都有

在yarn模式下，driver就是appmaster中的一个线程，

![1629877121004](大数据学习.assets/1629877121004.png)

![1629877341391](大数据学习.assets/1629877341391.png)



job切分为很多task，

### 常用命令

#### spark-shell

```shell
spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://hadoop102:7077 \
--executor-memory 2G \
--total-executor-cores 2 \
/opt/module/spark/examples/jars/spark-examples_2.11-2.1.1.jar \
10
```

| 参数                      | 解释                                                         | 可选值举例                                         |
| ------------------------- | ------------------------------------------------------------ | -------------------------------------------------- |
| --class                   | Spark程序中包含主函数的类                                    |                                                    |
| --master                  | Spark程序运行的模式                                          | 本地模式：local[*]、spark://hadoop102:7077、  Yarn |
| --executor-memory  1G     | 指定每个executor可用内存为1G                                 | 符合集群内存配置即可，具体情况具体分析。           |
| --total-executor-cores  2 | 指定**所有**executor使用的cpu核数为2个                       |                                                    |
| application-jar           | 打包好的应用jar，包含依赖。这个URL在集群中全局可见。 比如hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的path都包含同样的jar |                                                    |
| application-arguments     | 传给main()方法的参数                                         |                                                    |

默认端口

8080 spark-shell

4040 

8989 zookeeper管理spark端口 自定义

#### 转换

transformations，目标是对数据集进行操作，转换为新的rdd并返回其应用

```scala
textFile(path) //读文件夹下的所有文件



```

#### 执行

actions，

```scala
1.reduce(func): //通过函数func先聚集各分区的数据集，再聚集分区之间的数据，func接收两个参数，返回一个新值，新值再做为参数继续传递给函数func，直到最后一个元素
 
2.collect(): //以数据的形式返回数据集中的所有元素给Driver程序，为防止Driver程序内存溢出，一般要控制返回的数据集大小
 
3.count()：//返回数据集元素个数
 
4.first(): //返回数据集的第一个元素
 
5.take(n): //以数组的形式返回数据集上的前n个元素
 
6.top(n): //按默认或者指定的排序规则返回前n个元素，默认按降序输出
 
7.takeOrdered(n,[ordering]): //按自然顺序或者指定的排序规则返回前n个元素
```

### 运行模式

Spark有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。

yarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出。

```shell
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
./examples/jars/spark-examples_2.11-2.1.1.jar \
10
```

yarn-cluster：Driver程序运行在由ResourceManager启动的APPMaster适用于生产环境。

```shell
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode cluster \
./examples/jars/spark-examples_2.11-2.1.1.jar \
10
```

![1630302080496](大数据学习.assets/1630302080496.png)

![1630302097803](大数据学习.assets/1630302097803.png)

## 5.2、spark core

### 1. RDD

#### RDD基本概念

RDD算子在excutor执行，算子外的代码在Driver执行

RDD基于内存，封装的是计算逻辑、操作逻辑

RDD是对当前即将处理数据的一个抽象概念，为分布式的，以分区的形式存在于多个节点，执行transformation操作时，存在多个分区的一个RDD同时进行操作，转化为一个新的RDD

![1630321563417](大数据学习.assets/1630321563417.png)

每次transformation操作(如textFile、FlatMap、map、reducebykey)，都会生成新的包装的RDD，多次操作会生成多个RDD，维护其中次序关系的逻辑称为血缘关系

![1630318898216](大数据学习.assets/1630318898216.png)

#### RDD特性

RDD有很多实现类，每个RDD都 有五个特性

```
1 获取分区，get partition
2 分区的计算函数，处理分区的业务逻辑
3 依赖关系：宽依赖窄依赖
4 默认是哈希分区，根据键的哈希值分区 
5 
```

![1630321450881](大数据学习.assets/1630321450881.png)

算子

包括转换transformation和行动action

![1630321742772](大数据学习.assets/1630321742772.png)

### 2、一般性执行过程

```scala
object WordCount {
  def main(args: Array[String]): Unit = {

    //1、创建配置信息
    val conf=new SparkConf().setAppName("WC").setMaster("local[*]")
    //2、根据配置信息创建SparkContext，sc是spark程序入口
    val sc = new SparkContext(conf)
    //3、读文件，生成一个hadoopRdd
    val lineRdd:RDD[String] = sc.textFile("input")
    /**
     *  Return a new RDD by first applying a function to all elements of this
     *  RDD, and then flattening the results.
     */
    val wordRdd:RDD[String]=lineRdd.flatMap(line=>line.split(" "))

    
    val wordToOneRdd:RDD[(String,Int)] = wordRdd.map(word => (word, 1))
	  //map之后有一次落盘，防止多个分区的数据倾斜问题，一般等待的场景需要落盘
      //map shuffle 执行write
      /*******shuffle******/
      //reduce shuffle 执行read
    val wordToSumRdd:RDD[(String,Int)] = wordToOneRdd.reduceByKey((v1, v2) => v1 + v2)

    wordToSumRdd.saveAsTextFile("output")
	//执行action算子，执行之前的所有操作(之前的所有操作都是对rdd的包装)
    val wordToCountArray:Array[(String,Int)]=wordToSumRdd.collect()
    wordToCountArray.foreach(println)

    //sc.textFile(args(0)).flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect().foreach(println)

    sc.stop()
  }
}
```



# 扩展

通过反射创建新的类实例，有两种方法：Class.newInstance() 只能够调用无参的构造函数，即默认的构造函数； Constructor.newInstance() 可以根据传入的参数，调用任意构造构造函数。 

OLTP

NoSql

列存储

数据模型

# ----------数仓项目----------

# 一、日志数据采集

Nginx负责负载均衡，将请求尽可能平均的分配到各个服务器。

## 1、埋点设计

埋点数据一般是在前端直接采集

![1628051852151](大数据学习.assets/1628051852151.png)

先对需求信息进行设计，本项目日志分为两种：启动日志和事件日志：首先对于**启动日志格式设计**如下：

```xml
{
"ap":"xxxxx",//项目数据来源 app pc
"cm": {  //公共字段
		"mid": "",  // (String) 设备唯一标识
        "uid": "",  // (String) 用户标识
        "vc": "1",  // (String) versionCode，程序版本号
        "vn": "1.0",  // (String) versionName，程序版本名
        "l": "zh",  // (String) language系统语言
        "sr": "",  // (String) 渠道号，应用从哪个渠道来的。
        "os": "7.1.1",  // (String) Android系统版本
        "ar": "CN",  // (String) area区域
        "md": "BBB100-1",  // (String) model手机型号
        "ba": "blackberry",  // (String) brand手机品牌
        "sv": "V2.2.1",  // (String) sdkVersion
        "g": "",  // (String) gmail
        "hw": "1620x1080",  // (String) heightXwidth，屏幕宽高
        "t": "1506047606608",  // (String) 客户端日志产生时的时间
        "nw": "WIFI",  // (String) 网络模式
        "ln": 0,  // (double) lng经度
        "la": 0  // (double) lat 纬度
    },
"et":  [  //事件
            {
                "ett": "1506047605364",  //客户端事件产生时间
                "en": "display",  //事件名称
                "kv": {  //事件结果，以key-value形式自行定义
                    "goodsid": "236",
                    "action": "1",
                    "extend1": "1",
"place": "2",
"category": "75"
                }
            }
        ]
}

```

**事件日志**基本格式如下，表示一个用户的实践操作，其中et字段为事件，其中每个事件为一个json内容，可能有多个事件发生。

```xml
1540934156385|{
    "ap": "gmall", 
    "cm": {
        "uid": "1234", 
        "vc": "2", 
        "vn": "1.0", 
        "la": "EN", 
        "sr": "", 
        "os": "7.1.1", 
        "ar": "CN", 
        "md": "BBB100-1", 
        "ba": "blackberry", 
        "sv": "V2.2.1", 
        "g": "abc@gmail.com", 
        "hw": "1620x1080", 
        "t": "1506047606608", 
        "nw": "WIFI", 
        "ln": 0
    }, 
        "et": [
            {
                "ett": "1506047605364",  //客户端事件产生时间
                "en": "display",  //事件名称
                "kv": {  //事件结果，以key-value形式自行定义
                    "goodsid": "236",
                    "action": "1",
                    "extend1": "1",
"place": "2",
"category": "75"
                }
            },{
		        "ett": "1552352626835",
		        "en": "active_background",
		        "kv": {
			         "active_source": "1"
		        }
	        }
        ]
    }
}

```

## 2、日志生成

### 2.1. 需求分析

由于正常的日志文件需要由日志服务器生成，为方便起见，我们针对每个事件内容编写一个javabean，并通过写逻辑代码批量生成日志，并存储为json文件

![1628520561488](大数据学习.assets/1628520561488.png)

### 2.2. 新建项目

新建maven项目log.collector，java版本选择1.8

### 2.3. 导入依赖

在pom.xml文件添加：

其中fastjson框架 可以对bean对象信息和json相互转换

ch.qos.logback框架可以实现生成日志同时保存到指定目录的功能，还提供了日志过期清理等功能

```xml
<dependencies>
        <!--阿里巴巴开源json解析框架-->
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.51</version>
        </dependency>
        <!--日志生成框架-->
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <version>${logback.version}</version>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <version>${logback.version}</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.projectlombok/lombok -->
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <version>1.18.12</version>
            <scope>provided</scope>
        </dependency>

    </dependencies>
    <build>
        <plugins>
            <plugin>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>2.3.2</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
            <plugin>
                <artifactId>maven-assembly-plugin </artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                    <archive>
                        <manifest>
                            <mainClass>com.atguigu.appclient.AppMain</mainClass>
                        </manifest>
                    </archive>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>

    </build>
```

### 2.4. 日志配置

在resources文件夹下新建logback.xml文件

```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration debug="false">
    <!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径 -->
    <property name="LOG_HOME" value="/tmp/logs/" />

    <!-- 控制台输出 -->
    <appender name="STDOUT"
              class="ch.qos.logback.core.ConsoleAppender">
        <encoder
                class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符 -->
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- 按照每天生成日志文件。存储事件日志 -->
    <appender name="FILE"
              class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- <File>${LOG_HOME}/app.log</File>设置日志不超过${log.max.size}时的保存路径，注意，如果是web项目会保存到Tomcat的bin目录 下 -->
        <rollingPolicy
                class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!--日志文件输出的文件名 -->
            <FileNamePattern>${LOG_HOME}/app-%d{yyyy-MM-dd}.log</FileNamePattern>
            <!--日志文件保留天数 -->
            <MaxHistory>30</MaxHistory>
        </rollingPolicy>
        <encoder
                class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>%msg%n</pattern>
        </encoder>
        <!--日志文件最大的大小 -->
        <triggeringPolicy
                class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
            <MaxFileSize>10MB</MaxFileSize>
        </triggeringPolicy>
    </appender>

    <!--异步打印日志-->
    <appender name ="ASYNC_FILE" class= "ch.qos.logback.classic.AsyncAppender">
        <!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 -->
        <discardingThreshold >0</discardingThreshold>
        <!-- 更改默认的队列的深度,该值会影响性能.默认值为256 -->
        <queueSize>512</queueSize>
        <!-- 添加附加的appender,最多只能添加一个 -->
        <appender-ref ref = "FILE"/>
    </appender>

    <!-- 日志输出级别 -->
    <root level="INFO">
        <appender-ref ref="STDOUT" />
        <appender-ref ref="ASYNC_FILE" />
        <appender-ref ref="error" />
    </root>
</configuration>
```

### 2.5. 事件javabean

创建包com.atguigu.bean

[bean.zip](..\..\Work\JAVA\log.collector\src\main\java\com\atguigu\bean.zip) 

### 2.6. 主程序

创建包com.atguigu.appclient

逻辑说明

![1628522111727](大数据学习.assets/1628522111727.png)

```java
package com.atguigu.appclient;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONArray;
import com.alibaba.fastjson.JSONObject;
import com.atguigu.bean.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.UnsupportedEncodingException;
import java.util.Random;


/**
 * @ClassName 尚硅谷数仓：数据生成程序
 * @Description
 * @Author Wud
 * @Date 2021/8/5 0:32
 * @Version 1.0
 **/
public class AppMain {

    private final static Logger logger = LoggerFactory.getLogger(AppMain.class);
    private static Random rand = new Random();

    // 设备id
    private static int s_mid = 0;

    // 用户id
    private static int s_uid = 0;

    // 商品id
    private static int s_goodsid = 0;

    public static void main(String[] args) {

        // 参数一：控制发送每条的延时时间，默认是0
        Long delay = args.length > 0 ? Long.parseLong(args[0]) : 0L;

        // 参数二：循环遍历次数
        int loop_len = args.length > 1 ? Integer.parseInt(args[1]) : 1000;

        // 生成数据
        generateLog(delay, loop_len);
    }

    private static void generateLog(Long delay, int loop_len) {

        for (int i = 0; i < loop_len; i++) {

            int flag = rand.nextInt(2);

            switch (flag) {
                case (0):
                    //应用启动
                    AppStart appStart = generateStart();
                    String jsonString = JSON.toJSONString(appStart);

                    //控制台打印
                    logger.info(jsonString);
                    break;

                case (1):

                    JSONObject json = new JSONObject();

                    json.put("ap", "app");
                    json.put("cm", generateComFields());

                    JSONArray eventsArray = new JSONArray();

                    // 事件日志
                    // 商品点击，展示
                    if (rand.nextBoolean()) {
                        eventsArray.add(generateDisplay());
                        json.put("et", eventsArray);
                    }

                    // 商品详情页
                    if (rand.nextBoolean()) {
                        eventsArray.add(generateNewsDetail());
                        json.put("et", eventsArray);
                    }

                    // 商品列表页
                    if (rand.nextBoolean()) {
                        eventsArray.add(generateNewList());
                        json.put("et", eventsArray);
                    }

                    // 广告
                    if (rand.nextBoolean()) {
                        eventsArray.add(generateAd());
                        json.put("et", eventsArray);
                    }

                    // 消息通知
                    if (rand.nextBoolean()) {
                        eventsArray.add(generateNotification());
                        json.put("et", eventsArray);
                    }

                    // 用户后台活跃
                    if (rand.nextBoolean()) {
                        eventsArray.add(generateBackground());
                        json.put("et", eventsArray);
                    }

                    //故障日志
                    if (rand.nextBoolean()) {
                        eventsArray.add(generateError());
                        json.put("et", eventsArray);
                    }

                    // 用户评论
                    if (rand.nextBoolean()) {
                        eventsArray.add(generateComment());
                        json.put("et", eventsArray);
                    }

                    // 用户收藏
                    if (rand.nextBoolean()) {
                        eventsArray.add(generateFavorites());
                        json.put("et", eventsArray);
                    }

                    // 用户点赞
                    if (rand.nextBoolean()) {
                        eventsArray.add(generatePraise());
                        json.put("et", eventsArray);
                    }

                    //时间
                    long millis = System.currentTimeMillis();

                    //控制台打印
                    logger.info(millis + "|" + json.toJSONString());
                    break;
            }

            // 延迟
            try {
                Thread.sleep(delay);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
    }

    /**
     * 公共字段设置
     */
    private static JSONObject generateComFields() {

        AppBase appBase = new AppBase();

        //设备id
        appBase.setMid(s_mid + "");
        s_mid++;

        // 用户id
        appBase.setUid(s_uid + "");
        s_uid++;

        // 程序版本号 5,6等
        appBase.setVc("" + rand.nextInt(20));

        //程序版本名 v1.1.1
        appBase.setVn("1." + rand.nextInt(4) + "." + rand.nextInt(10));

        // 安卓系统版本
        appBase.setOs("8." + rand.nextInt(3) + "." + rand.nextInt(10));

        // 语言  es,en,pt
        int flag = rand.nextInt(3);
        switch (flag) {
            case (0):
                appBase.setL("es");
                break;
            case (1):
                appBase.setL("en");
                break;
            case (2):
                appBase.setL("pt");
                break;
        }

        // 渠道号   从哪个渠道来的
        appBase.setSr(getRandomChar(1));

        // 区域
        flag = rand.nextInt(2);
        switch (flag) {
            case 0:
                appBase.setAr("BR");
            case 1:
                appBase.setAr("MX");
        }

        // 手机品牌 ba ,手机型号 md，就取2位数字了
        flag = rand.nextInt(3);
        switch (flag) {
            case 0:
                appBase.setBa("Sumsung");
                appBase.setMd("sumsung-" + rand.nextInt(20));
                break;
            case 1:
                appBase.setBa("Huawei");
                appBase.setMd("Huawei-" + rand.nextInt(20));
                break;
            case 2:
                appBase.setBa("HTC");
                appBase.setMd("HTC-" + rand.nextInt(20));
                break;
        }

        // 嵌入sdk的版本
        appBase.setSv("V2." + rand.nextInt(10) + "." + rand.nextInt(10));
        // gmail
        appBase.setG(getRandomCharAndNumr(8) + "@gmail.com");

        // 屏幕宽高 hw
        flag = rand.nextInt(4);
        switch (flag) {
            case 0:
                appBase.setHw("640*960");
                break;
            case 1:
                appBase.setHw("640*1136");
                break;
            case 2:
                appBase.setHw("750*1134");
                break;
            case 3:
                appBase.setHw("1080*1920");
                break;
        }

        // 客户端产生日志时间
        long millis = System.currentTimeMillis();
        appBase.setT("" + (millis - rand.nextInt(99999999)));

        // 手机网络模式 3G,4G,WIFI
        flag = rand.nextInt(3);
        switch (flag) {
            case 0:
                appBase.setNw("3G");
                break;
            case 1:
                appBase.setNw("4G");
                break;
            case 2:
                appBase.setNw("WIFI");
                break;
        }

        // 拉丁美洲 西经34°46′至西经117°09；北纬32°42′至南纬53°54′
        // 经度
        appBase.setLn((-34 - rand.nextInt(83) - rand.nextInt(60) / 10.0) + "");
        // 纬度
        appBase.setLa((32 - rand.nextInt(85) - rand.nextInt(60) / 10.0) + "");

        return (JSONObject) JSON.toJSON(appBase);
    }

    /**
     * 商品展示事件
     */
    private static JSONObject generateDisplay() {

        AppDisplay appDisplay = new AppDisplay();

        boolean boolFlag = rand.nextInt(10) < 7;

        // 动作：曝光商品=1，点击商品=2，
        if (boolFlag) {
            appDisplay.setAction("1");
        } else {
            appDisplay.setAction("2");
        }

        // 商品id
        String goodsId = s_goodsid + "";
        s_goodsid++;

        appDisplay.setGoodsid(goodsId);

        // 顺序  设置成6条吧
        int flag = rand.nextInt(6);
        appDisplay.setPlace("" + flag);

        // 曝光类型
        flag = 1 + rand.nextInt(2);
        appDisplay.setExtend1("" + flag);

        // 分类
        flag = 1 + rand.nextInt(100);
        appDisplay.setCategory("" + flag);

        JSONObject jsonObject = (JSONObject) JSON.toJSON(appDisplay);

        return packEventJson("display", jsonObject);
    }

    /**
     * 商品详情页
     */
    private static JSONObject generateNewsDetail() {

        AppNewsDetail appNewsDetail = new AppNewsDetail();

        // 页面入口来源
        int flag = 1 + rand.nextInt(3);
        appNewsDetail.setEntry(flag + "");

        // 动作
        appNewsDetail.setAction("" + (rand.nextInt(4) + 1));

        // 商品id
        appNewsDetail.setGoodsid(s_goodsid + "");

        // 商品来源类型
        flag = 1 + rand.nextInt(3);
        appNewsDetail.setShowtype(flag + "");

        // 商品样式
        flag = rand.nextInt(6);
        appNewsDetail.setShowtype("" + flag);

        // 页面停留时长
        flag = rand.nextInt(10) * rand.nextInt(7);
        appNewsDetail.setNews_staytime(flag + "");

        // 加载时长
        flag = rand.nextInt(10) * rand.nextInt(7);
        appNewsDetail.setLoading_time(flag + "");

        // 加载失败码
        flag = rand.nextInt(10);
        switch (flag) {
            case 1:
                appNewsDetail.setType1("102");
                break;
            case 2:
                appNewsDetail.setType1("201");
                break;
            case 3:
                appNewsDetail.setType1("325");
                break;
            case 4:
                appNewsDetail.setType1("433");
                break;
            case 5:
                appNewsDetail.setType1("542");
                break;
            default:
                appNewsDetail.setType1("");
                break;
        }

        // 分类
        flag = 1 + rand.nextInt(100);
        appNewsDetail.setCategory("" + flag);

        JSONObject eventJson = (JSONObject) JSON.toJSON(appNewsDetail);

        return packEventJson("newsdetail", eventJson);
    }

    /**
     * 商品列表
     */
    private static JSONObject generateNewList() {

        AppLoading appLoading = new AppLoading();

        // 动作
        int flag = rand.nextInt(3) + 1;
        appLoading.setAction(flag + "");

        // 加载时长
        flag = rand.nextInt(10) * rand.nextInt(7);
        appLoading.setLoading_time(flag + "");

        // 失败码
        flag = rand.nextInt(10);
        switch (flag) {
            case 1:
                appLoading.setType1("102");
                break;
            case 2:
                appLoading.setType1("201");
                break;
            case 3:
                appLoading.setType1("325");
                break;
            case 4:
                appLoading.setType1("433");
                break;
            case 5:
                appLoading.setType1("542");
                break;
            default:
                appLoading.setType1("");
                break;
        }

        // 页面  加载类型
        flag = 1 + rand.nextInt(2);
        appLoading.setLoading_way("" + flag);

        // 扩展字段1
        appLoading.setExtend1("");

        // 扩展字段2
        appLoading.setExtend2("");

        // 用户加载类型
        flag = 1 + rand.nextInt(3);
        appLoading.setType("" + flag);

        JSONObject jsonObject = (JSONObject) JSON.toJSON(appLoading);

        return packEventJson("loading", jsonObject);
    }

    /**
     * 广告相关字段
     */
    private static JSONObject generateAd() {

        AppAd appAd = new AppAd();

        // 入口
        int flag = rand.nextInt(3) + 1;
        appAd.setEntry(flag + "");

        // 动作
        flag = rand.nextInt(5) + 1;
        appAd.setAction(flag + "");

        // 内容类型类型
        flag = rand.nextInt(6)+1;
        appAd.setContentType(flag+ "");

        // 展示样式
        flag = rand.nextInt(120000)+1000;
        appAd.setDisplayMills(flag+"");

        flag=rand.nextInt(1);
        if(flag==1){
            appAd.setContentType(flag+"");
            flag =rand.nextInt(6);
            appAd.setItemId(flag+ "");
        }else{
            appAd.setContentType(flag+"");
            flag =rand.nextInt(1)+1;
            appAd.setActivityId(flag+ "");
        }

        JSONObject jsonObject = (JSONObject) JSON.toJSON(appAd);

        return packEventJson("ad", jsonObject);
    }

    /**
     * 启动日志
     */
    private static AppStart generateStart() {

        AppStart appStart = new AppStart();

        //设备id
        appStart.setMid(s_mid + "");
        s_mid++;

        // 用户id
        appStart.setUid(s_uid + "");
        s_uid++;

        // 程序版本号 5,6等
        appStart.setVc("" + rand.nextInt(20));

        //程序版本名 v1.1.1
        appStart.setVn("1." + rand.nextInt(4) + "." + rand.nextInt(10));

        // 安卓系统版本
        appStart.setOs("8." + rand.nextInt(3) + "." + rand.nextInt(10));

        //设置日志类型
        appStart.setEn("start");

        //    语言  es,en,pt
        int flag = rand.nextInt(3);
        switch (flag) {
            case (0):
                appStart.setL("es");
                break;
            case (1):
                appStart.setL("en");
                break;
            case (2):
                appStart.setL("pt");
                break;
        }

        // 渠道号   从哪个渠道来的
        appStart.setSr(getRandomChar(1));

        // 区域
        flag = rand.nextInt(2);
        switch (flag) {
            case 0:
                appStart.setAr("BR");
            case 1:
                appStart.setAr("MX");
        }

        // 手机品牌 ba ,手机型号 md，就取2位数字了
        flag = rand.nextInt(3);
        switch (flag) {
            case 0:
                appStart.setBa("Sumsung");
                appStart.setMd("sumsung-" + rand.nextInt(20));
                break;
            case 1:
                appStart.setBa("Huawei");
                appStart.setMd("Huawei-" + rand.nextInt(20));
                break;
            case 2:
                appStart.setBa("HTC");
                appStart.setMd("HTC-" + rand.nextInt(20));
                break;
        }

        // 嵌入sdk的版本
        appStart.setSv("V2." + rand.nextInt(10) + "." + rand.nextInt(10));
        // gmail
        appStart.setG(getRandomCharAndNumr(8) + "@gmail.com");

        // 屏幕宽高 hw
        flag = rand.nextInt(4);
        switch (flag) {
            case 0:
                appStart.setHw("640*960");
                break;
            case 1:
                appStart.setHw("640*1136");
                break;
            case 2:
                appStart.setHw("750*1134");
                break;
            case 3:
                appStart.setHw("1080*1920");
                break;
        }

        // 客户端产生日志时间
        long millis = System.currentTimeMillis();
        appStart.setT("" + (millis - rand.nextInt(99999999)));

        // 手机网络模式 3G,4G,WIFI
        flag = rand.nextInt(3);
        switch (flag) {
            case 0:
                appStart.setNw("3G");
                break;
            case 1:
                appStart.setNw("4G");
                break;
            case 2:
                appStart.setNw("WIFI");
                break;
        }

        // 拉丁美洲 西经34°46′至西经117°09；北纬32°42′至南纬53°54′
        // 经度
        appStart.setLn((-34 - rand.nextInt(83) - rand.nextInt(60) / 10.0) + "");
        // 纬度
        appStart.setLa((32 - rand.nextInt(85) - rand.nextInt(60) / 10.0) + "");

        // 入口
        flag = rand.nextInt(5) + 1;
        appStart.setEntry(flag + "");

        // 开屏广告类型
        flag = rand.nextInt(2) + 1;
        appStart.setOpen_ad_type(flag + "");

        // 状态
        flag = rand.nextInt(10) > 8 ? 2 : 1;
        appStart.setAction(flag + "");

        // 加载时长
        appStart.setLoading_time(rand.nextInt(20) + "");

        // 失败码
        flag = rand.nextInt(10);
        switch (flag) {
            case 1:
                appStart.setDetail("102");
                break;
            case 2:
                appStart.setDetail("201");
                break;
            case 3:
                appStart.setDetail("325");
                break;
            case 4:
                appStart.setDetail("433");
                break;
            case 5:
                appStart.setDetail("542");
                break;
            default:
                appStart.setDetail("");
                break;
        }

        // 扩展字段
        appStart.setExtend1("");

        return appStart;
    }
    /**
     * 消息通知
     */
    private static JSONObject generateNotification() {

        AppNotification appNotification = new AppNotification();

        int flag = rand.nextInt(4) + 1;

        // 动作
        appNotification.setAction(flag + "");

        // 通知id
        flag = rand.nextInt(4) + 1;
        appNotification.setType(flag + "");

        // 客户端弹时间
        appNotification.setAp_time((System.currentTimeMillis() - rand.nextInt(99999999)) + "");

        // 备用字段
        appNotification.setContent("");

        JSONObject jsonObject = (JSONObject) JSON.toJSON(appNotification);

        return packEventJson("notification", jsonObject);
    }

    /**
     * 后台活跃
     */
    private static JSONObject generateBackground() {

        AppActive_background appActive_background = new AppActive_background();

        // 启动源
        int flag = rand.nextInt(3) + 1;
        appActive_background.setActive_source(flag + "");

        JSONObject jsonObject = (JSONObject) JSON.toJSON(appActive_background);

        return packEventJson("active_background", jsonObject);
    }

    /**
     * 错误日志数据
     */
    private static JSONObject generateError() {

        AppErrorLog appErrorLog = new AppErrorLog();

        String[] errorBriefs = {"at cn.lift.dfdf.web.AbstractBaseController.validInbound(AbstractBaseController.java:72)", "at cn.lift.appIn.control.CommandUtil.getInfo(CommandUtil.java:67)"};        //错误摘要
        String[] errorDetails = {"java.lang.NullPointerException\\n    " + "at cn.lift.appIn.web.AbstractBaseController.validInbound(AbstractBaseController.java:72)\\n " + "at cn.lift.dfdf.web.AbstractBaseController.validInbound", "at cn.lift.dfdfdf.control.CommandUtil.getInfo(CommandUtil.java:67)\\n " + "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n" + " at java.lang.reflect.Method.invoke(Method.java:606)\\n"};        //错误详情

        //错误摘要
        appErrorLog.setErrorBrief(errorBriefs[rand.nextInt(errorBriefs.length)]);
        //错误详情
        appErrorLog.setErrorDetail(errorDetails[rand.nextInt(errorDetails.length)]);

        JSONObject jsonObject = (JSONObject) JSON.toJSON(appErrorLog);

        return packEventJson("error", jsonObject);
    }

    /**
     * 为各个事件类型的公共字段（时间、事件类型、Json数据）拼接
     */
    private static JSONObject packEventJson(String eventName, JSONObject jsonObject) {

        JSONObject eventJson = new JSONObject();

        eventJson.put("ett", (System.currentTimeMillis() - rand.nextInt(99999999)) + "");
        eventJson.put("en", eventName);
        eventJson.put("kv", jsonObject);

        return eventJson;
    }

    /**
     * 获取随机字母组合
     *
     * @param length 字符串长度
     */
    private static String getRandomChar(Integer length) {

        StringBuilder str = new StringBuilder();
        Random random = new Random();

        for (int i = 0; i < length; i++) {
            // 字符串
            str.append((char) (65 + random.nextInt(26)));// 取得大写字母
        }

        return str.toString();
    }

    /**
     * 获取随机字母数字组合
     * @param length 字符串长度
     */
    private static String getRandomCharAndNumr(Integer length) {

        StringBuilder str = new StringBuilder();
        Random random = new Random();

        for (int i = 0; i < length; i++) {

            boolean b = random.nextBoolean();

            if (b) { // 字符串
                // int choice = random.nextBoolean() ? 65 : 97; 取得65大写字母还是97小写字母
                str.append((char) (65 + random.nextInt(26)));// 取得大写字母
            } else { // 数字
                str.append(String.valueOf(random.nextInt(10)));
            }
        }

        return str.toString();
    }

    /**
     * 收藏
     */
    private static JSONObject generateFavorites() {

        AppFavorites favorites = new AppFavorites();

        favorites.setCourse_id(rand.nextInt(10));
        favorites.setUserid(rand.nextInt(10));
        favorites.setAdd_time((System.currentTimeMillis() - rand.nextInt(99999999)) + "");

        JSONObject jsonObject = (JSONObject) JSON.toJSON(favorites);

        return packEventJson("favorites", jsonObject);
    }

    /**
     * 点赞
     */
    private static JSONObject generatePraise() {

        AppPraise praise = new AppPraise();

        praise.setId(rand.nextInt(10));
        praise.setUserid(rand.nextInt(10));
        praise.setTarget_id(rand.nextInt(10));
        praise.setType(rand.nextInt(4) + 1);
        praise.setAdd_time((System.currentTimeMillis() - rand.nextInt(99999999)) + "");

        JSONObject jsonObject = (JSONObject) JSON.toJSON(praise);

        return packEventJson("praise", jsonObject);
    }

    /**
     * 评论
     */
    private static JSONObject generateComment() {

        AppComment comment = new AppComment();

        comment.setComment_id(rand.nextInt(10));
        comment.setUserid(rand.nextInt(10));
        comment.setP_comment_id(rand.nextInt(5));

        comment.setContent(getCONTENT());
        comment.setAddtime((System.currentTimeMillis() - rand.nextInt(99999999)) + "");

        comment.setOther_id(rand.nextInt(10));
        comment.setPraise_count(rand.nextInt(1000));
        comment.setReply_count(rand.nextInt(200));

        JSONObject jsonObject = (JSONObject) JSON.toJSON(comment);

        return packEventJson("comment", jsonObject);
    }

    /**
     * 生成单个汉字
     */
    private static char getRandomChar() {

        String str = "";
        int hightPos; //
        int lowPos;

        Random random = new Random();

        //随机生成汉子的两个字节
        hightPos = (176 + Math.abs(random.nextInt(39)));
        lowPos = (161 + Math.abs(random.nextInt(93)));

        byte[] b = new byte[2];
        b[0] = (Integer.valueOf(hightPos)).byteValue();
        b[1] = (Integer.valueOf(lowPos)).byteValue();

        try {
            str = new String(b, "GBK");
        } catch (UnsupportedEncodingException e) {
            e.printStackTrace();
            System.out.println("错误");
        }

        return str.charAt(0);
    }

    /**
     * 拼接成多个汉字
     */
    private static String getCONTENT() {

        StringBuilder str = new StringBuilder();

        for (int i = 0; i < rand.nextInt(100); i++) {
            str.append(getRandomChar());
        }

        return str.toString();
    }

}
```

项目打包

点击右侧maven-lifecycle-package

等待下方提示打包结束后，在项目target目录下找到带依赖的jar包 [log.collector-1.0-SNAPSHOT-jar-with-dependencies.jar](..\..\Work\JAVA\log.collector\target\log.collector-1.0-SNAPSHOT-jar-with-dependencies.jar) 

## 3、环境搭建

### 3.1. hadoop配置压缩格式

<img src="大数据学习.assets/1628336681567.png" alt="1628336681567" style="zoom:50%;" />

上传lzo到/opt/module/hadoop-3.1.3/share/hadoop/common/  

 [hadoop-lzo-0.4.20.jar](..\..\迅雷下载\尚硅谷大数据2020.08\12，离线数仓项目\1.电商采集平台项目\2.资料\01_jar\02_hadoop(linux编译过)\hadoop-lzo-0.4.20.jar) 

修改hadoop配置文件

```shell
cd /opt/module/hadoop-3.1.3/etc/hadoop
vim core-site.xml
```

```xml
添加如下配置
<configuration>
<property>
<name>io.compression.codecs</name>
<value>
org.apache.hadoop.io.compress.GzipCodec,
org.apache.hadoop.io.compress.DefaultCodec,
org.apache.hadoop.io.compress.BZip2Codec,
org.apache.hadoop.io.compress.SnappyCodec,
com.hadoop.compression.lzo.LzoCodec,
com.hadoop.compression.lzo.LzopCodec
</value>
</property>

<property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
</property>
</configuration>
```

同步jar包及配置文件

```shell
cd /opt/module/hadoop-3.1.3/share/hadoop/common/  
xsync hadoop-lzo-0.4.20.jar

cd /opt/module/hadoop-3.1.3/etc/hadoop
xsync core-site.xml
```

### 3.2. zookeeper配置集群启动

```shell
进入目录：
cd /home/atguigu/
vim zk.sh 
```

```shell
写入：
#!/bin/bash
case $1 in
"start"){
	for i in hadoop102 hadoop103 hadoop104
	do
		echo ==============$i==============
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start"
	done
};;
"stop"){
	for i in hadoop102 hadoop103 hadoop104
	do
		echo ==============$i==============
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop"
	done
};;
"status"){
	for i in hadoop102 hadoop103 hadoop104
	do
		echo ==============$i==============
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status"
	done
};;
esac
```

```shell
修改权限
chmod 777 zk.sh
```

### 3.3. 日志集群启动

将1.2打包的jar文件，上传到/opt/module/目录下

编写启动脚本 vim /home/atguigu/bin/lg.sh

```shell
#!/bin/bash
for i in hadoop102 hadoop103
do
	echo ==============$i generate logs============== 
 ssh $i "java -jar /opt/module/log.collector-1.0-SNAPSHOT-jar-with-dependencies.jar >/dev/null 2>&1"
done

```

修改权限

```shell
chmod 777 /home/atguigu/bin/lg.sh
```

分发脚本及jar包

```shell
xsync /home/atguigu/bin/

xsync /opt/module/log.collector-1.0-SNAPSHOT-jar-with-dependencies.jar
```

### 3.4. 同步时间修改脚本

```shell
#使用方式
dt.sh 2021-08-11
```

```shell
进入目录：
cd /home/atguigu/
vim dt.sh 

写入：
#!/bin/bash

for i in hadoop102 hadoop103 hadoop104
do
    echo "========== $i =========="
    ssh -t $i "sudo date -s $1"
done

修改权限
chmod 777 dt.sh

分发
xsync /home/atguigu/bin/
```

### 3.5. 集群控制脚本

```shell
进入目录：
cd /home/atguigu/
vim xcall.sh 

写入：
#!/bin/bash

for i in hadoop102 hadoop103 hadoop104
do
    echo --------- $i ----------
    ssh $i "$*"
done


修改权限
chmod 777 xcall.sh

分发
xsync /home/atguigu/bin/

使用
xcall.sh 指令
如 xcall.sh jps
```

## 4、flume配置

### 4.1. flume原理

**Source**

负责接收数据，可以处理各种类型各种格式的日志数据，包括         avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy  。

Exec Source可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。

Spooling Directory Source监控目录，不支持断点续传。

Taildir source维护了一个索引 offset 因此支持断点续传。此外还有多目录。

batchSize 控制每次拉取数据的个数，负责读取数据的速率

**Channel**

**--原生channel**

分为Memory Channel和File Channel

Memory Channel存储在内存中，断电可能丢失，但速度快，容量是100个event

File Channel向硬盘持久化，比较安全，传输速度稍慢，容量是100万个event

memory channel + kafka sink 速度慢于 kafka channel

**--kafka channel**

如果选用kafka channel，并且下一级是kafka，可以直接将数据存到kafka中，比较快速。

美中不足是传输数据前面都带个topic前缀，即：topic + 数据内容。因此后续需要把topic前缀删除掉。

书写配置文件时，parseAsFlumeEvent如果选为false时，可以默认将topic删除掉。 在Flume1.6版本之前有bug，无法删除。1.7版本后做了修复，可以正常删除。

**--生产环境选择**

如果是普通日志，追求效率，选择Memory Channel，丢数据不影响大距，京东日活pb(丢几百万条)

如果是金融的数据或者和钱有关系的数据，不允许丢，选择filechannel

**Sink**

Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。

Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。

### 4.2. 常用使用流程

```shell
在flume/jobs路径下书写项目文件，并定义agent、source、channel、sink

cd /opt/module/flume/jobs
vim flume-netcat-logger.conf

这是最简单的一个案例，添加内容如下：
# Name the components on this agent
# a1为定义的agent名称
a1.sources = r1  #r1表示a1的Source的名称
a1.sinks = k1  #k1表示a1的Sink的名称
a1.channels = c1  #c1表示a1的Channel的名称，可以有复数个

# Describe/configure the source
a1.sources.r1.type = netcat #设置数据来源为netcat
a1.sources.r1.bind = localhost  #监听的主机为本机
a1.sources.r1.port = 44444  #监听的端口号位44444

# Describe the sink
a1.sinks.k1.type = logger  #输出为控制台logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory  #选择memory channel
a1.channels.c1.capacity = 1000  #总容量为1000个event
a1.channels.c1.transactionCapacity = 100 #每接收100个提交一次

# Bind the source and sink to the channel
# 将输入输出与通道连接起来
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

```shell
执行启动命令(注意提前添加环境变量)

方式一
flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console

方式二
flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console

参数说明：
	--conf/-c：表示配置文件存储在conf/目录
	--name/-n：表示给agent起名为a1
	--conf-file/-f：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。
	-Dflume.root.logger=INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。
```

### 4.3. 配置文件编写流程

```shell
##定义agent组成 source、channel、sink
a1.sources=r1 r2
a1.channels=c1 c2
a1.sinks=k1 k2

##定义source

##定义channel

##定义sink

## 拼装
```

### 4.4. 配置生产通道

![1628525389716](大数据学习.assets/1628525389716.png)

##### 1. 编写配置文件

| hdfs.fileType | SequenceFile | File format: currently `SequenceFile`, `DataStream` or `CompressedStream` (1)DataStream will not compress output file and please don’t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC |
| ------------- | ------------ | ------------------------------------------------------------ |
|               |              |                                                              |

```shell
vim file-flume-kafka.conf

a1.sources=r1
a1.channels=c1 c2

# configure source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/module/flume/test/log_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /tmp/logs/app.+
a1.sources.r1.fileHeader = true
a1.sources.r1.channels = c1 c2

#interceptor
a1.sources.r1.interceptors =  i1 i2
a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.LogETLInterceptor$Builder
a1.sources.r1.interceptors.i2.type = com.atguigu.flume.interceptor.LogTypeInterceptor$Builder

a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_start = c1
a1.sources.r1.selector.mapping.topic_event = c2

# configure channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c1.kafka.topic = topic_start
a1.channels.c1.parseAsFlumeEvent = false
a1.channels.c1.kafka.consumer.group.id = flume-consumer

a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c2.kafka.topic = topic_event
a1.channels.c2.parseAsFlumeEvent = false
a1.channels.c2.kafka.consumer.group.id = flume-consumer
```

##### 2. 写拦截器java

本项目中自定义了两个拦截器，分别是：ETL拦截器、日志类型区分拦截器。

ETL拦截器主要用于，过滤时间戳不合法和Json数据不完整的日志

日志类型区分拦截器主要用于，将启动日志和事件日志区分开来，方便发往Kafka的不同Topic。

写拦截器步骤

定义类--实现intercept接口 重写四个方法

初始化 关闭 单event 多event Builder(静态内部类)

###### pom.xml

```xml
pom.xml

    <dependencies>
        <dependency>
            <groupId>org.apache.flume</groupId>
            <artifactId>flume-ng-core</artifactId>
            <version>1.9.0</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>2.3.2</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
```

###### LogETLInterceptor

用于数据清洗

```java
package com.atguigu.flume.interceptor;

import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.Charset;
import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.List;

/**
 * @ClassName LogETLInterceptor
 * @Description
 * @Author Wud
 * @Date 2021/8/9 18:38
 * @Version 1.0
 **/
public class LogETLInterceptor implements Interceptor {
    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {
        byte[] body = event.getBody();
        String log = new String(body, StandardCharsets.UTF_8);

        if (log.contains("start")){
            //启动日志
            if (LogUtils.validateStart(log)){
                return event;
            }
        }else {
            //事件日志
            if (LogUtils.validateEvent(log)){
                return event;
            }
        }

        //既不是启动日志，也不是事件日志，返回null
        return null;
    }

    @Override
    public List<Event> intercept(List<Event> events) {
        ArrayList<Event> interceptors=new ArrayList<>();
        for (Event event : events) {
            //调用前面定义的单event方法intercept
            Event intercept1 = intercept(event);
            //如果不需清洗，就加入到列表中
            //等于null的，被清洗掉
            if (intercept1!=null){
                interceptors.add(intercept1);
            }
        }
        return interceptors;
    }

    @Override
    public void close() {

    }
    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new LogETLInterceptor();
        }

        @Override
        public void configure(Context context) {

        }
    }
}
```

###### LogUtils

用于数据清洗的逻辑实现

```java
package com.atguigu.flume.interceptor;

import org.apache.commons.lang.math.NumberUtils;

/**
 * @ClassName LogUtils
 * @Description
 * @Author Wud
 * @Date 2021/8/9 18:51
 * @Version 1.0
 **/
public class LogUtils {
    /*{"action":"1","ar":"MX","ba":"HTC","detail":"542","en":"sta
rt","entry":"2","extend1":"","g":"S3HQ7LKM@gmail.com","hw":"640
*960","l":"en","la":"-43.4","ln":"-98.3","loading_time":"10","m
d":"HTC-5","mid":"993","nw":"WIFI","open_ad_type":"1","os":"8.2
.1","sr":"D","sv":"V2.9.0","t":"1559551922019","uid":"993","vc"
:"0","vn":"1.1.5"}*/
    // 清洗启动日志
    public static boolean validateStart(String log) {
        //日志为空直接返回
        if (log==null)return false;
        //日志如果不以{开始，或不以}结束，都返回
        if (!log.trim().startsWith("{")||!log.trim().endsWith("}"))return false;
        //都没问题，返回true
        return true;
    }

    //清洗事件日志
    public static boolean validateEvent(String log) {
        if (log==null)return false;
        // 时间戳|json
        //  \|正则表达式|，\表示转义，因此用\\|
        String[] logContents = log.split("\\|");
        //如果没时间戳，直接返回
        if (logContents.length!=2){
            return false;
        }
        //如果开始的事件戳不为13位数字，或者根本不是数字，返回
        if (logContents[0].length()!=13||!NumberUtils.isDigits(logContents[0]))return false;
        //如果json内容开头不是{，返回
        if (!logContents[1].trim().startsWith("{")||!logContents[1].trim().endsWith("}"))return false;
        //都没问题，返回true
        return true;
    }
}
```

###### LogTypeInterceptor

用于日志区分，将start日志和event日志分别处理

```java
package com.atguigu.flume.interceptor;

import java.lang.String;

import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * @ClassName LogTypeInterceptor
 * @Description
 * @Author Wud
 * @Date 2021/8/9 19:07
 * @Version 1.0
 **/
public class LogTypeInterceptor implements Interceptor {
    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {
        byte[] body = event.getBody();
        String log = new String(body, Charset.forName("UTF-8"));

        //取出header对象
        //这里相当于一个文件指针(方法定义)，不同于正常的返回值
        Map<String,String> header=event.getHeaders();

        if (log.contains("start")){
            //对header做标记，与flume配置文件一致
            header.put("topic","topic_start");
        }else {
            header.put("topic","topic_event");
        }
        return event;
    }

    @Override
    public List<Event> intercept(List<Event> list) {
        ArrayList<Event> interceptors=new ArrayList<>();
        for (Event event : list) {
            Event intercept = intercept(event);
            interceptors.add(intercept);
        }
        return interceptors;

    }

    @Override
    public void close() {

    }
    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new LogTypeInterceptor();
        }

        @Override
        public void configure(Context context) {

        }
    }
}
```

##### 3. flume采集集群启动脚本

```shell
#启动命令
fl.sh start
#停止命令
fl.sh stop
```

```shell
cd /home/atguigu/bin
vim fl.sh

#! /bin/bash

case $1 in
"start"){
        for i in hadoop102 hadoop103
        do
                echo " --------启动 $i 采集flume-------"
                ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/test1 2>&1  &"
        done
};;	
"stop"){
        for i in hadoop102 hadoop103
        do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
        done

};;
esac

说明1：nohup，该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup就是不挂起的意思，不挂断地运行命令。
说明2：awk 默认分隔符为空格
说明3：xargs 表示取出前面命令运行的结果，作为后面命令的输入参数。
```

```shell
#修改权限
chmod 777 fl.sh

#集群分发
xsync /home/atguigu/bin
```

### 4.5. 配置消费通道

```shell
cd /opt/module/flume/conf 
vim kafka-flume-hdfs.conf
```

##### 1. 编写配置文件

```shell
##定义agent组成 source、channel、sink
a1.sources=r1 r2
a1.channels=c1 c2
a1.sinks=k1 k2

##定义source
# source1
a1.sources.r1.type=org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics = topic_start
# source2
a1.sources.r2.type=org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.batchSize = 5000 #每次向channel写多少个数据
a1.sources.r2.batchDurationMillis = 2000  #不管凑没凑够batchSize，每隔2000ms都向channel提交一次
a1.sources.r2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r2.kafka.topics = topic_event

##定义channel
#channel1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1/ #设置多目录能提高性能
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 6 #source如果向channel传递失败，隔6秒再传递一次
#channel2
a1.channels.c2.type = file
a1.channels.c2.checkpointDir = /opt/module/flume/checkpoint/behavior2
a1.channels.c2.dataDirs = /opt/module/flume/data/behavior2/
a1.channels.c2.maxFileSize = 2146435071
a1.channels.c2.capacity = 1000000
a1.channels.c2.keep-alive = 6

##定义sink
# sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_start/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = logstart-
#sink2
a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = /origin_data/gmall/log/topic_event/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = logevent-

## 不要产生大量小文件(优化用)
#等待多少秒滚动生成新文件
a1.sinks.k1.hdfs.rollInterval = 20
#或 没到达134217728字节大小(128MB)，产生一个新文件
a1.sinks.k1.hdfs.rollSize = 134217728
#或 写多少个event产生一个新文件
a1.sinks.k1.hdfs.rollCount = 0

a1.sinks.k2.hdfs.rollInterval = 20
a1.sinks.k2.hdfs.rollSize = 134217728
a1.sinks.k2.hdfs.rollCount = 0

##企业标准
#等待多少秒产生下一个文件 1h
#a1.sinks.k1.hdfs.rollInterval = 3600
#或 没多少字节大小，产生一个新文件 128MB
#a1.sinks.k1.hdfs.rollSize = 134217728
#或 写多少个event产生一个新文件
#a1.sinks.k1.hdfs.rollCount = 0

## 控制输出文件是原生文件的lzo压缩。
a1.sinks.k1.hdfs.fileType = CompressedStream 
a1.sinks.k2.hdfs.fileType = CompressedStream 

a1.sinks.k1.hdfs.codeC = lzop
a1.sinks.k2.hdfs.codeC = lzop

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1

a1.sources.r2.channels = c2
a1.sinks.k2.channel= c2

```

##### 2. flume消费集群启动脚本

```shell
#启动命令
fl2.sh start
#停止命令
fl2.sh stop
```

```shell
#编写消费通道脚本
cd /home/atguigu/bin
vim fl2.sh
```

```shell
#! /bin/bash

case $1 in
"start"){
        for i in hadoop104
        do
                echo " --------启动 $i 消费flume-------"
                ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log.txt   2>&1 &"
        done
};;
"stop"){
        for i in hadoop104
        do
                echo " --------停止 $i 消费flume-------"
                ssh $i "ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '{print \$2}' | xargs -n1 kill"
        done

};;
esac
```

```shell
#修改脚本权限
chmod 777 fl2.sh
```

### 4.6. 集群启动脚本

```shell
cd /home/atguigu/bin
vim cluster.sh
```

```shell
#! /bin/bash

case $1 in
"start"){
	echo " -------- 启动 集群 -------"
	echo " -------- 启动 hadoop集群 -------"
	/opt/module/hadoop-3.1.3/sbin/start-dfs.sh 
	ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
	#启动 Zookeeper集群
	zk.sh start
sleep 4s;
	#启动 Flume采集集群
	fl.sh start
	#启动 Kafka采集集群
	kf.sh start
sleep 6s;
	#启动 Flume消费集群
	fl2.sh start
	};;
"stop"){
    echo " -------- 停止 集群 -------"
    #停止 Flume消费集群
	fl2.sh stop

	#停止 Kafka采集集群
	kf.sh stop

    sleep 6s;

	#停止 Flume采集集群
	fl.sh stop

	#停止 Zookeeper集群
	zk.sh stop

	echo " -------- 停止 hadoop集群 -------"
	ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
	/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh 
};;
esac

```





## 5、Kafka配置

### 5.1. 安装

上传-解压到/opt/module/kafka

创建log文件夹 mkdir /opt/module/kafka/logs

修改配置文件

```shell
cd config/

vim server.properties

输入以下内容：
#broker的全局唯一编号，不能重复
broker.id=0
#删除topic功能使能
delete.topic.enable=true
#修改kafka运行日志存放的路径
log.dirs=/opt/module/kafka/logs
#修改配置连接Zookeeper集群地址
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka
```

配置环境变量

```shell
[atguigu@hadoop102 module]$ sudo vi /etc/profile.d/my_env.sh

#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin

[atguigu@hadoop102 module]$ source /etc/profile.d/my_env.sh

```

分发软件包及环境变量

```shell
xsync /opt/module/kafka
xsync /etc/profile.d/my_env.sh
```

修改hadoop103、hadoop104的broker.id

```shell
在这两台主机输入
vim /opt/module/kafka/config/server.properties
将broker.id分别修改为1 2
```

### 5.2. 启动

##### 单机命令

```shell
#启动
kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties

#关闭
kafka-server-stop.sh
```

##### 集群命令

群起命令

```shell
#启动
kf.sh start

#停止
kf.sh stop
```

群起配置

```shell
cd /home/atguigu/bin
vim kf.sh

#! /bin/bash
case $1 in
"start"){
    for i in hadoop102 hadoop103 hadoop104
    do
        echo " --------启动 $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
    done
};;
"stop"){
    for i in hadoop102 hadoop103 hadoop104
    do
        echo " --------停止 $i Kafka-------"
        ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop"
    done
};;
esac
```

修改权限

```shell
chmod 777 kf.sh
```

### 5.3. 常用命令

#### 5.3.1. kafka-topic.sh

```shell
查看当前服务器中的所有topic：
#旧版本通过zookeeper查看
kafka-topics.sh --zookeeper hadoop102:2181/kafka --list
#新版本内置查看
kafka-topics.sh -bootstrap-server hadoop102:9092 --list


解析：
--zookeeper hadoop102:9092/kafka #是config/server.properties中定义的zookeeper地址
--list #对所有节点进行列表显示
```

```shell
创建topic：
#旧版本
kafka-topics.sh --zookeeper hadoop102:2181/kafka \--create --replication-factor 3 --partitions 1 --topic second
#新版本内置查看
kafka-topics.sh --bootstrap-server hadoop102:9092 --create --topic my_topic_name --partitions 20 --replication-factor 3 --config x=y

解析：
--zookeeper hadoop102:2181/kafka #与前面一致
\--create #创建操作
--replication-factor 3 #副本数为3
--partitions 1 #分区数为1
--topic second #主题名称定义为second
```

```shell
删除名称为first的topic
#要保证没有生产者或者消费者使用该topic(flume不能开)
kafka-topics.sh --zookeeper hadoop102:2181/kafka --delete --topic first
#新版本
kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first

--zookeeper hadoop102:9092/kafka #与前面一致
\--delete #删除操作
--topic second #主题名称定义为second
```

```shell
查看某个topic详情
#旧版本
kafka-topics.sh --zookeeper hadoop102:2181/kafka --describe --topic first
#新版本
kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first

#显示该topic的属性(创建topic时设置)
输出
Topic: first	PartitionCount: 1	ReplicationFactor: 3	Configs: 
Topic: first	Partition: 0	Leader: 1	Replicas: 2,0,1	Isr: 1,2,0
```

```shell
修改topic分区数
#旧版本
kafka-topics.sh --zookeeper hadoop102:2181/kafka --alter --topic first --partitions 6

#新版本修改分区
kafka-topics.sh --bootstrap-server broker_host:port --alter --topic my_topic_name --partitions 40
```

#### 5.3.2. 生产者与消费者

```shell
发送消息
kafka-console-producer.sh --broker-list hadoop102:9092 --topic first
```

```shell
消费消息
#监听该topic消息，每收到一个，消费一个
kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first

#显示该topic所有历史消息，并继续监听
kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first --from-beginning
```

### 5.4. 原理解析

#### 5.4.1. 基础架构

![1628599157177](大数据学习.assets/1628599157177.png)

（1）Producer ：消息生产者，就是向kafka broker发消息的客户端；
（2）Consumer ：消息消费者，向kafka broker取消息的客户端；
（3）Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的所有数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。
（4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。
（5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；
（6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；
（7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。
（8）leader：每个分区多个副本的“主”，生产者发送数据的对象都是leader。
（9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。

#### 5.4.2. topic

![1628597125467](大数据学习.assets/1628597125467.png)

kafka中消息按topic分类，topic是逻辑概念，partition是物理概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。

所有消息日志存储目录由config/server.properties中的log.dir属性配置，每个topic显示为一个路径，后面的编号为分区号，如果仅一个分区(partition)，则会显示topic名字-0；如果多个分区，会编号-1 -2 -3...，如图。

![1628597072893](大数据学习.assets/1628597072893.png)

进入topic目录内部，里面内容按照索引分片为不同segment，每个segment包含：消息日志.log文件；内容索引文件.index文件，指向文件中每个message的物理偏移地址。index和log文件以当前segment的第一条消息的offset命名 。

#### 5.4.3. 生产者

**分区原则**

原因：分区可以并发传输，提高读写效率，不同区可以在不同节点进行配置。

我们需要将producer发送的数据封装成一个**ProducerRecord**对象  

（1）指明 partition 的情况下，直接将指明的值直接作为 partiton 值；

（2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；

（3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。

**数据可靠性保证**

​         为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。  

![1628598135076](大数据学习.assets/1628598135076.png)

![1628598205482](大数据学习.assets/1628598205482.png)

ISR



# 二、业务数据采集

sku 具体的某一规格的产品，比如iphone x 128G

spu 表示一类手机，比如iphone x，可以共用商品图片，海报，商品属性等

共27个表

![1628751232142](大数据学习.assets/1628751232142.png)

每个表向外的箭头，表示要关注哪个属性(对应的id)

如果预先没有表关系图，需要根据id信息推导出表关系图

##  1、同步策略

### 全量同步策略

每日对数据进行全量导入，并依据日期作为一个分区，每日分区的数据都做保留。

使用时直接查询最新日期的数据(相当于**覆盖**)

适用于表数据量不大，每天既有新数据插入，也有旧数据修改的场景

如：3.10日对品牌表数据全量导入，3.11再全量导入，最后HDFS中存储的是所有分区(日期)的品牌表数据

![1628753125760](大数据学习.assets/1628753125760.png)

### 增量同步策略

每日增量，每天导入一份当日的新增数据，作为一个分区。(仅新增，不对以前数据做修改，一般用于记录流水账)

使用时，对所有分区join处理(相当于**追加**)

适用于表数据量大，每天只有新数据插入的场景

![1628753487784](大数据学习.assets/1628753487784.png)

### 新增及变化策略

每日新增及变化，就是存储创建时间和操作时间(修改时间)都是今天的数据

使用时直接查询创建时间或操作时间

适用于表数据量大，但既有新增也有变化的场景

如：要找3.11日的数据，就要把之前创建，3.11日修改的订单；3.11日新增的订单，都导入HDFS中

![1628753787057](大数据学习.assets/1628753787057.png)

### 特殊策略

一次性导入的，不需要遵守同步策略

如：日期、地区

## 2、业务数据导入

### 2.1、生成Mysql数据

新建目录db_log

```shell
mkdir /opt/module/db_log
```

上传本地jar包到db_log目录

```
本地：D:\迅雷下载\尚硅谷大数据2020.08\12，离线数仓项目\1.电商采集平台项目\2.资料\02_数据库生成脚本
找到gmall-mock-db-2020-03-16-SNAPSHOT.jar和 application.properties文件
```

修改application.properties配置文件

```shell
#业务日期
mock.date=2021-08-11
#是否重置，如果是第一次导入数据，置1
mock.clear=1
```

启动脚本导入第一天数据到hadoop102的mysql

```shell
java -jar gmall-mock-db-2020-03-16-SNAPSHOT.jar
```

再修改application.properties配置文件

```shell
#业务日期，改为第二天日期
mock.date=2021-08-12
#是否重置，不是第一次导入数据，置0
mock.clear=0
```

启动脚本导入第二天数据

```shell
java -jar gmall-mock-db-2020-03-16-SNAPSHOT.jar
```

### 2.2、上传数据到hdfs

#### 1. 写Sqoop脚本

在/home/atguigu/bin目录下创建文件

```shell
vim gmall_mysql_to_hdfs.sh
```

```SHELL
#!/bin/bash
#定义变量
sqoop="/opt/module/sqoop/bin/sqoop"

#获取时间
#每天00：30分执行，延时30分钟，为了前日数据都已经存储完毕，共kafka把数据全上传到hdfs上，称作T+1模式
if [ -n "$2"];then #判断如果$2非空
	do_date=$2
else #否则算一下前一天日期
	do_date=`date -d '-1 day' +%F`	
fi

#具体业务逻辑
# 输入参数是哪个表名字，我就导入哪个表；
# 或者输入first，将所有27张表导入hdfs
# 或者输入all，地区和省份没必要每次都导，导剩下的25张

import_data(){
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/gmall \
--username root \
--password 000000 \
--target-dir /origin_data/gmall/db/$1/$do_date \
--delete-target-dir \ #sqoop要求目标路径不能已经存在，故删除
--query "$2 and  \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

#对导入的数据直接创建lzo索引
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/gmall/db/$1/$do_date
}

import_order_info(){
  import_data order_info "select
                            id, 
                            final_total_amount, 
                            order_status, 
                            user_id, 
                            out_trade_no, 
                            create_time, 
                            operate_time,
                            province_id,
                            benefit_reduce_amount,
                            original_total_amount,
                            feight_fee      
                        from order_info
                        where (date_format(create_time,'%Y-%m-%d')='$do_date' 
                        or date_format(operate_time,'%Y-%m-%d')='$do_date')"
}

import_coupon_use(){
  import_data coupon_use "select
                          id,
                          coupon_id,
                          user_id,
                          order_id,
                          coupon_status,
                          get_time,
                          using_time,
                          used_time
                        from coupon_use
                        where (date_format(get_time,'%Y-%m-%d')='$do_date'
                        or date_format(using_time,'%Y-%m-%d')='$do_date'
                        or date_format(used_time,'%Y-%m-%d')='$do_date')"
}

import_order_status_log(){
  import_data order_status_log "select
                                  id,
                                  order_id,
                                  order_status,
                                  operate_time
                                from order_status_log
                                where date_format(operate_time,'%Y-%m-%d')='$do_date'"
}

import_activity_order(){
  import_data activity_order "select
                                id,
                                activity_id,
                                order_id,
                                create_time
                              from activity_order
                              where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_user_info(){
  import_data "user_info" "select 
                            id,
                            name,
                            birthday,
                            gender,
                            email,
                            user_level, 
                            create_time,
                            operate_time
                          from user_info 
                          where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                          or DATE_FORMAT(operate_time,'%Y-%m-%d')='$do_date')"
}

import_order_detail(){
  import_data order_detail "select 
                              od.id,
                              order_id, 
                              user_id, 
                              sku_id,
                              sku_name,
                              order_price,
                              sku_num, 
                              od.create_time  
                            from order_detail od
                            join order_info oi
                            on od.order_id=oi.id
                            where DATE_FORMAT(od.create_time,'%Y-%m-%d')='$do_date'"
}

import_payment_info(){
  import_data "payment_info"  "select 
                                id,  
                                out_trade_no, 
                                order_id, 
                                user_id, 
                                alipay_trade_no, 
                                total_amount,  
                                subject, 
                                payment_type, 
                                payment_time 
                              from payment_info 
                              where DATE_FORMAT(payment_time,'%Y-%m-%d')='$do_date'"
}

import_comment_info(){
  import_data comment_info "select
                              id,
                              user_id,
                              sku_id,
                              spu_id,
                              order_id,
                              appraise,
                              comment_txt,
                              create_time
                            from comment_info
                            where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_order_refund_info(){
  import_data order_refund_info "select
                                id,
                                user_id,
                                order_id,
                                sku_id,
                                refund_type,
                                refund_num,
                                refund_amount,
                                refund_reason_type,
                                create_time
                              from order_refund_info
                              where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

#固定语法 where 1=1必须写(因为写脚本时，大多都有where条件(\$CONDITIONS" \)，为了统一写)
import_sku_info(){
  import_data sku_info "select 
                          id,
                          spu_id,
                          price,
                          sku_name,
                          sku_desc,
                          weight,
                          tm_id,
                          category3_id,
                          create_time
                        from sku_info where 1=1"
}

import_base_category1(){
  import_data "base_category1" "select 
                                  id,
                                  name 
                                from base_category1 where 1=1"
}

import_base_category2(){
  import_data "base_category2" "select
                                  id,
                                  name,
                                  category1_id 
                                from base_category2 where 1=1"
}

import_base_category3(){
  import_data "base_category3" "select
                                  id,
                                  name,
                                  category2_id
                                from base_category3 where 1=1"
}

import_base_province(){
  import_data base_province "select
                              id,
                              name,
                              region_id,
                              area_code,
                              iso_code
                            from base_province
                            where 1=1"
}

import_base_region(){
  import_data base_region "select
                              id,
                              region_name
                            from base_region
                            where 1=1"
}

import_base_trademark(){
  import_data base_trademark "select
                                tm_id,
                                tm_name
                              from base_trademark
                              where 1=1"
}

import_spu_info(){
  import_data spu_info "select
                            id,
                            spu_name,
                            category3_id,
                            tm_id
                          from spu_info
                          where 1=1"
}

import_favor_info(){
  import_data favor_info "select
                          id,
                          user_id,
                          sku_id,
                          spu_id,
                          is_cancel,
                          create_time,
                          cancel_time
                        from favor_info
                        where 1=1"
}

import_cart_info(){
  import_data cart_info "select
                        id,
                        user_id,
                        sku_id,
                        cart_price,
                        sku_num,
                        sku_name,
                        create_time,
                        operate_time,
                        is_ordered,
                        order_time
                      from cart_info
                      where 1=1"
}

import_coupon_info(){
  import_data coupon_info "select
                          id,
                          coupon_name,
                          coupon_type,
                          condition_amount,
                          condition_num,
                          activity_id,
                          benefit_amount,
                          benefit_discount,
                          create_time,
                          range_type,
                          spu_id,
                          tm_id,
                          category3_id,
                          limit_num,
                          operate_time,
                          expire_time
                        from coupon_info
                        where 1=1"
}

import_activity_info(){
  import_data activity_info "select
                              id,
                              activity_name,
                              activity_type,
                              start_time,
                              end_time,
                              create_time
                            from activity_info
                            where 1=1"
}

import_activity_rule(){
    import_data activity_rule "select
                                    id,
                                    activity_id,
                                    condition_amount,
                                    condition_num,
                                    benefit_amount,
                                    benefit_discount,
                                    benefit_level
                                from activity_rule
                                where 1=1"
}

import_base_dic(){
    import_data base_dic "select
                            dic_code,
                            dic_name,
                            parent_code,
                            create_time,
                            operate_time
                          from base_dic
                          where 1=1" 
}

case $1 in
  "order_info")
     import_order_info
;;
  "base_category1")
     import_base_category1
;;
  "base_category2")
     import_base_category2
;;
  "base_category3")
     import_base_category3
;;
  "order_detail")
     import_order_detail
;;
  "sku_info")
     import_sku_info
;;
  "user_info")
     import_user_info
;;
  "payment_info")
     import_payment_info
;;
  "base_province")
     import_base_province
;;
  "base_region")
     import_base_region
;;
  "base_trademark")
     import_base_trademark
;;
  "activity_info")
      import_activity_info
;;
  "activity_order")
      import_activity_order
;;
  "cart_info")
      import_cart_info
;;
  "comment_info")
      import_comment_info
;;
  "coupon_info")
      import_coupon_info
;;
  "coupon_use")
      import_coupon_use
;;
  "favor_info")
      import_favor_info
;;
  "order_refund_info")
      import_order_refund_info
;;
  "order_status_log")
      import_order_status_log
;;
  "spu_info")
      import_spu_info
;;
  "activity_rule")
      import_activity_rule
;;
  "base_dic")
      import_base_dic
;;

"first")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_province
   import_base_region
   import_base_trademark
   import_activity_info
   import_activity_order
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
;;
"all")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_trademark
   import_activity_info
   import_activity_order
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
;;
esac



```

#### 2. 导入到HDFS命令

```shell
#初次导入
gmall_mysql_to_hdfs.sh first 2021-08-11

#每日导入
gmall_mysql_to_hdfs.sh all 2021-08-12
```

# 数据采集平台总结

## 生成日志数据

### 1、一键生成流程(shell)

```shell
##使用命令

#生成指定日期的log文件并上传到HDFS
flume manage.sh getlog 2021-08-11
#打开flume的生产通道和消费通道
flume manage.sh start
#关闭通道
flume manage.sh stop
```

```shell
cd /home/atguigu/bin
vim flume_manage.sh

#!/bin/bash
case $1 in
"start"){
    fl.sh start
    fl2.sh start
};;
"stop"){
    fl.sh stop
    fl2.sh stop
};;
"getlog"){
    #if[ -n "$2"]
    #then
        fl.sh stop
        fl2.sh stop
       sleep 4s;
        dt.sh $2
       sleep 4s;
        fl.sh start
        fl2.sh start
       sleep 6s;
        lg.sh
    #fi
};;
esac

chmod 777 flume_manage.sh
```



### 2、常规流程

```shell
#1、修改集群时间到指定一天
dt.sh 2021-08-11
#2、打开集群，尤其是flume通道
fl.sh start #输入通道
fl2.sh start #输出通道
#3、启动日志生成脚本
lg.sh

#数据会通过生产者Flume通道-Kafka-消费者Flume通道存储到HDFS的orgin_data/gmall/log中
```

**问题解决：**

如果无法导入数据，说明kafka已经导入过比当前日期更新的数据，因此无法导入旧数据(时间正常情况下不能倒流)

解决办法

```shell
#1、关闭flume通道
fl.sh stop #输入通道
fl2.sh stop #输出通道
#2、删除Kafka的topic
kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic topic-start
kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic topic-event
#3、启动flume通道
fl.sh start #输入通道
fl2.sh start #输出通道
#4、启动日志生成脚本
lg.sh
```

## 生成业务数据

### 生成数据到Mysql

**已经生成过了的话不用反复生成**

修改application.properties配置文件

```shell
#业务日期
mock.date=2021-08-11
#是否重置，如果是第一次导入数据，置1
mock.clear=1
```

启动脚本导入第一天数据到hadoop102的mysql

```shell
java -jar gmall-mock-db-2020-03-16-SNAPSHOT.jar
```

再修改application.properties配置文件

```shell
#业务日期，改为第二天日期
mock.date=2021-08-12
#是否重置，不是第一次导入数据，置0
mock.clear=0
```

启动脚本导入第二天数据

```shell
java -jar gmall-mock-db-2020-03-16-SNAPSHOT.jar
```

### 上传数据到hdfs

```shell
#初次导入
gmall_mysql_to_hdfs.sh first 2021-08-11

#每日导入
gmall_mysql_to_hdfs.sh all 2021-08-12
```

# 三、离线数仓

数仓的用途：存储、管理、分析、计算，用于支持BI系统。BI系统商务智能，利用数据挖掘出数据价值，去指导业务决策

## 1、基本知识梳理

### 1.1. 关系建模与维度建模

OLTP：联机事务管理。普通的实时数据库增删改查，业务系统数据处理。采用关系模型。

OLAP：联机分析管理，多维分析。大数据场景，对大量历史数据的分析。采用维度模型。

关系模型：严格遵循第三范式。业务系统的一个javabean，对应数据库的一张表。ER-entity relitation。目标是去除表的所有冗余信息(即完全按照第三范式设计)，主要用于业务系统写javabean。缺点是如果用于数据库，表太细碎，查询的时候得大量join。

维度建模：以事实表为中心进行表的组织，存在数据冗余，但表结构简单，易于查询。主要包含事实表和维度表。还可分为三种模型：星型模型、星座模型、雪花模型

星型模型：一个事实表为中心，多个维度表关联。减少join就是减少shufflle，因此性能最好，但冗余最多。

<img src="大数据学习.assets/1629023546644.png" alt="1629023546644" style="zoom:50%;" />

雪花模型：一个事实表为中心，多个维度表与之关联，同时每个维度表还有自己的维度表。本质上是在消除维度表中的数据冗余。性能一般，但冗余较少。

<img src="大数据学习.assets/1629023559142.png" alt="1629023559142" style="zoom:50%;" />

星座模型：多个事实表，事实表之间可能存在维度表共享。属于星型模型的扩展，最常用。

<img src="大数据学习.assets/1629023696096.png" alt="1629023696096" style="zoom:50%;" />

### 1.2. 事实表和维度表

#### 维度表

每张维度表对应现实世界的一个对象或者概念，例如：用户、商品、日期、地区。通常小于<10万条。内容相对固定。

![1629077812722](大数据学习.assets/1629077812722.png)

#### 事实表

**常用事务型事实表，每个事件对应一个事实表，表中每一行代表一次业务事件的一条记录(每行一条操作记录)，每一行包括：度量值、与维度表相连接的外键**，通常具有两个及以上的外键，外键之间表示维度表之间多对多的关系。

事实这个术语表示的是业务事件里的度量值(可以统计、可以计算的字段)。例如订单事件中的下单金额。

特点是：数据量非常的大(行多)、列比较少、经常发生变化，每天都有新增。

与维度表的联系，即带着维度去统计事实表的度量值，通俗的讲，维度类似于观察事物的角度，比如针对订单事实表，要统计：每个日期的销售额、每个产品的销售额、地区的销售额，前缀叫做维度、或者角度，后面的销售额就是事实表的度量值。

```mysql
#例：统计每一年的销售额：将事实表与对应维度表join，再按年份分组，最后求sum
select sum(SalesAmount) 
from SalesOrder
join Date 
on SalesOrder.DateId==Date.DateId 
group by YearId
```

![1629078117713](大数据学习.assets/1629078117713.png)

##### 事务型事实表

 以**每个事务或事件为单位**，例如一个销售订单记录，一笔支付记录等，作为事实表里的一行数据。**一旦事务被提交，事实表数据被插入，数据就不再进行更改**，其更新方式为**增量更新。**  

##### 周期型快照事实表

适用于关心某个度量值的结果，而不是过程变化。类似每日全量表。

周期型快照事实表中**不会保留所有数据**，**只保留固定时间间隔的数据**，例如每天或者每月的销售额，或每月的账户余额等。

##### 累积型快照事实表

**一行数据按照状态更新，不断地持续的向里面写数据。累计快照事实表用于跟踪业务事实的变化。**例如，数据仓库中可能需要累积或者存储订单从下订单开始，到订单商品被打包、运输、和签收的各个业务阶段的时间点数据来跟踪订单声明周期的进展情况。当这个业务过程进行时，事实表的记录也要不断更新。

查出这条数据所在分区，对所有需要修改数据在select中进行修改，再insert overwrite

![1629079706947](大数据学习.assets/1629079706947.png)

怎么建表：哪些表：哪些字段；表关系

## 2、数仓分层

### 2.1、原始数据

25个表：23个数据表 2个用户行为日志(事件日志(几种事件对应几种表)、启动日志)

### 2.2、ODS层

对原始数据层的数据完全对应，保持数据原貌

hive,采用load，装载原始数据层的数据，把表映射到指定路径

相当于剪切操作

### 2.3、DWD层

对ODS层数据进行select，再insert到DWD层中

同时对事件日志进行解析

**表信息讲解：**

订单相关：订单表+订单明细表

订单信息表里存的都是编号，具体信息对应哪个要到base_dic里面查看，base_dic里面都是键值对

订单状态日志：保存订单状态的变化，流水表。用于做累积型快照事实表

order_redund_info ：退款信息

![1627963180169](大数据学习.assets/1627963180169.png)

#### 数据处理四部曲

四部曲：选择业务过程、声明粒度、确认维度、确认事实

**选择业务过程：**

明确我们想要哪些事实表

**声明粒度：**

一定要选择最小粒度，即最明细的数据

评论：事务性事实表，用户、评论的商品作为粒度

收藏：周期性快照事实表，商品id，被收藏个数，

**确认维度：**

观察角度(想统计哪一类数据 )，每个维度是一个外键

时间、地区、用户、性别、商品、活动(可能按活动统计订单)、优惠券

**确认事实的度量值：**

这个表也称为：业务总线矩阵

![1629080915215](大数据学习.assets/1629080915215.png)

![1627965597393](大数据学习.assets/1627965597393.png)

**维度建模**

把跟该维度相关的所有表合并成一个大表，称作维度退化

#### 用户行为日志

把每种事件解析为一种表

### 2.4、DWS层

分区表

DWS\DWT是对数据结构的优化，DWD层都是明细数据，数据量太大，可能出现重复计算

因此预先搭建了常用指标的数据层

以主题对象建模，主题是分析问题的角度

基本思路是：比如我想要订单的地区总金额以及订单关于地区的汇总，此时我们根据度量值(订单金额)找到订单信息表，将订单信息表与地区表进行join，如果我想统计地区信息，就count(*) 再 group by 地区，如果我想统计金额，就sum(value)再group by地区。这里的订单的地区总金额就可以是地区主题的一个字段。

| 事实\维度      | **时间** | **用户** | **地区** | **商品** | **优惠券** | **活动** | **编码** | **度量值** |
| -------------- | -------- | -------- | -------- | -------- | ---------- | -------- | -------- | ---------- |
| **订单**       | √        | √        | √        |          |            | √        |          | 件数/金额  |
| **订单详情**   | √        |          | √        | √        |            |          |          | 件数/金额  |
| **支付**       | √        |          | √        |          |            |          |          | 金额       |
| **加购**       | √        | √        |          | √        |            |          |          | 件数/金额  |
| **收藏**       | √        | √        |          | √        |            |          |          | 个数       |
| **评价**       | √        | √        |          | √        |            |          |          | 个数       |
| **退款**       | √        | √        |          | √        |            |          |          | 件数/金额  |
| **优惠券领用** | √        | √        |          |          | √          |          |          | 个数       |

主题和维度对应，**每个维度对应一个表有几个度量值，对应的主题宽表就有几个字段，里面是对每个事实的各个度量值的统计值。**

比如地区主题，有在每个地区的：订单的件数，订单的总金额，订单金额，订单的优惠金额，支付金额

比如用户主题，有每个用户的：......

统计的是各个主题对象当天的行为

### 2.5、DWT层

大表

统计各个主题在某个时间段的累积行为，不同于DWS只对当天活跃的进行统计

DWT存储的是全量的数据

### 2.6、ADS层

## 3、环境搭建

搭建hive on spark

(写的是HQL，实际运行的是基于yarn的spark)

(反之是spark on hive，写的是sparkSQL，底层运行的是hive)

[官方文档] [Hive on Spark: Getting Started - Apache Hive - Apache Software Foundation](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3a+Getting+Started) 

编译主要是解决hive 和 spark的兼容性问题，下载源码，修改其中的pom.xml依赖改为自己本机的版本，重新打包

spark运行模式：spark on yarn\spark alone\spark on cluster

```shell
 ./dev/make-distribution.sh --name without-hive --tgz -Pyarn -Phadoop-3.1 -Dhadoop.version=3.1.3 -Pparquet-provided -Porc-provided -Phadoop-provided
 
 --name without-hive 名字后面加without-hive，以为集群已经配置好hive了，不需要重复配置，所以这个配置里没有hive依赖(不知道哪句话把hive依赖排除的)
 --tgz 后缀为tgz
 -Pyarn 表示运行模式为在yarn集群上运算，带有yarn的依赖
 -Phadoop-3.1 用hadoop3.1架构
 -Dhadoop.version=3.1.3 自定义依赖，用来覆盖自带的pom.xml中的依赖
 -Pparquet-provided -Porc-provided -Phadoop-provided 编译的时候加载这些依赖，打包时不加载这些依赖，即说明已经安装了这些依赖，不需要再次配置
```

hiveserver2 用于启动beeline客户端

metastore 用于提供元数据的服务，

hive访问元数据有两种，一种hive客户端查询，一种元数据服务metastore ；

如果在hive-site.xml中配置了hive.metastore,那么客户端启动后会访问元数据服务metastore，metastore访问mysql

## 4、数仓搭建

## 4.1、ODS

分区表 LZO压缩

数据load装载

``字段名或者表明是关键字时，需要反引号

### 日志数据表

#### ods_start_log表

以下都是hive操作

```mysql
#建表，ods_start_log

drop table if exists ods_start_log;
CREATE EXTERNAL TABLE ods_start_log (`line` string)
PARTITIONED BY (`dt` string)
STORED AS
#输入是lzo压缩文件
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
#hive表的默认输出就是这个
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/warehouse/gmall/ods/ods_start_log';
```

自动化装载数据的数仓，每日定时调度，把sql封装为shell脚本。

写sql时以一天数据为目标，放到shell里再遍历即可。

数据装载

```mysql
#剪切数据，hive执行。命令执行完后，/origin_data/gmall/log/topic_start/2021-08-11的数据会被剪切到/warehouse/gmall/ods/ods_start_log表单映射的HDFS路径下

load data inpath '/origin_data/gmall/log/topic_start/2021-08-11' overwrite into table gmall.ods_start_log partition(dt='2021-08-11');

#创建lzo索引，shell执行
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_start_log/dt=2021-08-11
```

#### ods_event_log表

```mysql
#建表

drop table if exists ods_event_log;
CREATE EXTERNAL TABLE ods_event_log(`line` string)
PARTITIONED BY (`dt` string)
STORED AS
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/warehouse/gmall/ods/ods_event_log';
```

```mysql
load data inpath '/origin_data/gmall/log/topic_event/2021-08-11' overwrite into table gmall.ods_event_log partition(dt='2021-08-11');
```

```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=2021-08-11
```

#### 装载脚本

将上面两个表的装载命令封装为shell脚本

```shell
cd /home/atguigu/bin/
vim hdfs_to_ods_log.sh
```

```shell
#!/bin/bash

db=gmall
hive=/opt/module/hive/bin/hive
do_date=`date -d '-1 day' +%F`  #日期减一，%F以年-月-日，即2021-08-11形式显示

if [[ -n "$1" ]]; then #如果参数非空，则赋值覆盖
    do_date=$1
fi

sql="
load data inpath '/origin_data/gmall/log/topic_start/$do_date' overwrite into table ${db}.ods_start_log partition(dt='$do_date');
load data inpath '/origin_data/gmall/log/topic_event/$do_date' overwrite into table ${db}.ods_event_log partition(dt='$do_date');
"

$hive -e "$sql" #执行数据装载命令

#创建lzo索引
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_start_log/dt=$do_date
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_event_log/dt=$do_date
```

### 业务数据表

分区表、LZO、字段(与数据库字段一致)

建表时的分隔符row format delimited fields terminated by '\t'，要与sqoop导入数据设置的分隔符--fields-terminated-by '\t' \保持一致

## 4.2、DWD层(日志数据)

lzop：自带索引，支持切片

lzo：不支持索引，不支持切片

hive自带解析json字符串的函数

解析事件日志get_json_object

```mysql
select get_json_object('[select * from gmall.ods_start_log limit 1]',"$[0]");
```



### 启动日志

#### 创建启动表

解析启动日志：每个KEY映射为一个字段即可

```mysql
#表单属性与日志的key一一对应
#stored as parquet转换为列式存储，parquet支持切片
#TBLPROPERTIES('parquet.compression'='lzo');表单属性：列式存储的内容压缩格式为lzo，
drop table if exists dwd_start_log;
CREATE EXTERNAL TABLE dwd_start_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
`app_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`entry` string, 
`open_ad_type` string, 
`action` string, 
`loading_time` string, 
`detail` string, 
`extend1` string
)
PARTITIONED BY (dt string)
stored as parquet 
location '/warehouse/gmall/dwd/dwd_start_log/'
TBLPROPERTIES('parquet.compression'='lzo');
```

#### 启动数据装载

```mysql
insert overwrite table dwd_start_log
PARTITION (dt='2021-08-11')
select 
    get_json_object(line,'$.mid') mid_id,
    get_json_object(line,'$.uid') user_id,
    get_json_object(line,'$.vc') version_code,
    get_json_object(line,'$.vn') version_name,
    get_json_object(line,'$.l') lang,
    get_json_object(line,'$.sr') source,
    get_json_object(line,'$.os') os,
    get_json_object(line,'$.ar') area,
    get_json_object(line,'$.md') model,
    get_json_object(line,'$.ba') brand,
    get_json_object(line,'$.sv') sdk_version,
    get_json_object(line,'$.g') gmail,
    get_json_object(line,'$.hw') height_width,
    get_json_object(line,'$.t') app_time,
    get_json_object(line,'$.nw') network,
    get_json_object(line,'$.ln') lng,
    get_json_object(line,'$.la') lat,
    get_json_object(line,'$.entry') entry,
    get_json_object(line,'$.open_ad_type') open_ad_type,
    get_json_object(line,'$.action') action,
    get_json_object(line,'$.loading_time') loading_time,
    get_json_object(line,'$.detail') detail,
    get_json_object(line,'$.extend1') extend1
from ods_start_log 
where dt='2021-08-11';
```

#### 启动日志装载脚本

```shell
cd /home/atguigu/bin/
vim ods_to_dwd_start_log.sh

#!/bin/bash

# 定义变量方便修改
APP=gmall
hive=/opt/module/hive/bin/hive

# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
	do_date=$1
else 
	do_date=`date -d "-1 day" +%F`  
fi 

sql="
insert overwrite table ${APP}.dwd_start_log
PARTITION (dt='${do_date}')
select 
    get_json_object(line,'$.mid') mid_id,
    get_json_object(line,'$.uid') user_id,
    get_json_object(line,'$.vc') version_code,
    get_json_object(line,'$.vn') version_name,
    get_json_object(line,'$.l') lang,
    get_json_object(line,'$.sr') source,
    get_json_object(line,'$.os') os,
    get_json_object(line,'$.ar') area,
    get_json_object(line,'$.md') model,
    get_json_object(line,'$.ba') brand,
    get_json_object(line,'$.sv') sdk_version,
    get_json_object(line,'$.g') gmail,
    get_json_object(line,'$.hw') height_width,
    get_json_object(line,'$.t') app_time,
    get_json_object(line,'$.nw') network,
    get_json_object(line,'$.ln') lng,
    get_json_object(line,'$.la') lat,
    get_json_object(line,'$.entry') entry,
    get_json_object(line,'$.open_ad_type') open_ad_type,
    get_json_object(line,'$.action') action,
    get_json_object(line,'$.loading_time') loading_time,
    get_json_object(line,'$.detail') detail,
    get_json_object(line,'$.extend1') extend1
from ${APP}.ods_start_log 
where dt='${do_date}';
"

$hive -e "$sql"

```

### 业务日志

#### 基本格式

```json
#时间戳
1628611305351|{
    #公共字段
    "cm":{
        "ln":"-114.9",
        "sv":"V2.7.7",
        "os":"8.1.4",
        "g":"44C17D72@gmail.com",
        "mid":"1",
        "nw":"3G",
        "l":"es",
        "vc":"0",
        "hw":"640*960",
        "ar":"MX",
        "uid":"1",
        "t":"1628512130880",
        "la":"26.7",
        "md":"HTC-14",
        "vn":"1.3.7",
        "ba":"HTC",
        "sr":"G"
    },
    "ap":"app",
	#事件字段(难点所在)
    "et":[
        {
            "ett":"1628565594763",
            "en":"display",
            "kv":{
                "goodsid":"0",
                "action":"2",
                "extend1":"1",
                "place":"1",
                "category":"67"
            }
        },
        {
            "ett":"1628532146441",
            "en":"newsdetail",
            "kv":{
                "entry":"2",
                "goodsid":"1",
                "news_staytime":"35",
                "loading_time":"5",
                "action":"2",
                "showtype":"5",
                "category":"83",
                "type1":""
            }
        },
        {
            "ett":"1628559734073",
            "en":"loading",
            "kv":{
                "extend2":"",
                "loading_time":"27",
                "action":"1",
                "extend1":"",
                "type":"3",
                "type1":"542",
                "loading_way":"2"
            }
        },
        {
            "ett":"1628543841075",
            "en":"active_background",
            "kv":{
                "active_source":"3"
            }
        },
        {
            "ett":"1628515968936",
            "en":"error",
            "kv":{
                "errorDetail":"java.lang.NullPointerException\\n    at cn.lift.appIn.web.AbstractBaseController.validInbound(AbstractBaseController.java:72)\\n at cn.lift.dfdf.web.AbstractBaseController.validInbound",
                "errorBrief":"at cn.lift.dfdf.web.AbstractBaseController.validInbound(AbstractBaseController.java:72)"
            }
        },
        {
            "ett":"1628516667592",
            "en":"praise",
            "kv":{
                "target_id":5,
                "id":0,
                "type":1,
                "add_time":"1628527009673",
                "userid":9
            }
        }
    ]
}
```

#### 解析目标

因为每一个行业务日志，在ods_event_log中仅为一个字符串，因此到达DWD层时需要对其进行解析。

且由于含有时间戳，不同于常规意义的json文件，无法直接用get_json_object函数处理。

数据主要包含时间戳、cm公共字段集合、app字段、et事件字段集合，其中前三者均含有限个数且固定的字段，而et事件其中有未知个数的事件，每个事件的字段个数不同。

因此目标是根据不同的事件，将数据插入到不同的事件表中。

第一步

先对数据进行拆解(explode)，拆解后的数据为一行变成多行，每一行包括公共字段，事件种类(type)、事件名称(event)。如图上到中所示：

![1629128109552](大数据学习.assets/1629128109552.png)

![1629127906137](大数据学习.assets/1629127906137.png)

实现方式为编写UDF函数

lateral view explode()

```mysql
UDTF、一对多
#解析事件
输入：事件数组(json数组的字符串)
输出：多行，2列(标记，事件)
将不同事件放入不同表中
```

```mysql
UDF、一对一
#解析时间戳和cm公共字段
输入：line(事件日志)，key
输出：key对应的value
```

第二步

对拆解后的数据进行处理，分发插入到不同的表单中。

```mysql
#假设编写的udf函数就叫udf、udtf函数就叫udtf,line是ods_event_log原数据的一行
select
 udf(line,'ln'),
 udf(line,'mid'),
 
 
 type,
 event,
 udf(line,'st')  #server_time
from ods_event_log lateral view udtf(udf(line,'et')) tmp as type,event;
```

### 业务日志拆分到临时表

#### 编写UDF和UDTF

```hive
hive (gmall)>
use gmall;
create function base_analizer as 'com.atguigu.gmall.hive.LogUDF' using jar 'hdfs://hadoop102:8020/user/hive/jars/hive-202108-1.0-SNAPSHOT.jar';

create function flat_analizer as 'com.atguigu.gmall.hive.LogUDTF' using jar 'hdfs://hadoop102:8020/user/hive/jars/hive-202108-1.0-SNAPSHOT.jar'; 
```

永久函数和临时函数

临时函数与数据库无关

永久函数与表类似，使用时在gmall数据库，可以直接用函数名。如果在别的数据库用，需要：gmall.函数名

查询函数

show functions;

![1630076899341](大数据学习.assets/1630076899341.png)

如果hive自定义函数写错了，只需要打包替换原来路径的jar包，如这里是hdfs:/user/hive/jars  

```xml
    <repositories>
        <repository>
            <id>spring</id>
            <url></url>
        </repository>
    </repositories>
```

中间表：公共字段、事件标记、事件日志、时间戳

```mysql
select
    base_analizer(line,'mid'),
    base_analizer(line,'uid'),
    ...
    event_name,
    event_json,
    base_analizer(line,'st'),
from ods_event_log lateral view flat_analizer(base_analizer(line,'et'))tmp as event_name,event_json
where dt='2021-08-11'
```

```mysql
set mapreduce.job.queuename=hive;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
insert overwrite table dwd_base_event_log partition(dt='2021-08-11')
select
    base_analizer(line,'mid') as mid_id,
    base_analizer(line,'uid') as user_id,
    base_analizer(line,'vc') as version_code,
    base_analizer(line,'vn') as version_name,
    base_analizer(line,'l') as lang,
    base_analizer(line,'sr') as source,
    base_analizer(line,'os') as os,
    base_analizer(line,'ar') as area,
    base_analizer(line,'md') as model,
    base_analizer(line,'ba') as brand,
    base_analizer(line,'sv') as sdk_version,
    base_analizer(line,'g') as gmail,
    base_analizer(line,'hw') as height_width,
    base_analizer(line,'t') as app_time,
    base_analizer(line,'nw') as network,
    base_analizer(line,'ln') as lng,
    base_analizer(line,'la') as lat,
    event_name,
    event_json,
    base_analizer(line,'st') as server_time
from ods_event_log lateral view flat_analizer(base_analizer(line,'et')) tmp_flat as event_name,event_json
where dt='2021-08-11' and base_analizer(line,'et')<>'';

```

#### 写解析脚本

解析到中间表

```shell
 cd ~/bin
 vim ods_to_dwd_base_event_log.sh


#!/bin/bash

# 定义变量方便修改
APP=gmall
hive=/opt/module/hive/bin/hive

# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
	do_date=$1
else 
	do_date=`date -d "-1 day" +%F`  
fi 

sql="
use gmall;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
insert overwrite table ${APP}.dwd_base_event_log partition(dt='$do_date')
select
    ${APP}.base_analizer(line,'mid') as mid_id,
    ${APP}.base_analizer(line,'uid') as user_id,
    ${APP}.base_analizer(line,'vc') as version_code,
    ${APP}.base_analizer(line,'vn') as version_name,
    ${APP}.base_analizer(line,'l') as lang,
    ${APP}.base_analizer(line,'sr') as source,
    ${APP}.base_analizer(line,'os') as os,
    ${APP}.base_analizer(line,'ar') as area,
    ${APP}.base_analizer(line,'md') as model,
    ${APP}.base_analizer(line,'ba') as brand,
    ${APP}.base_analizer(line,'sv') as sdk_version,
    ${APP}.base_analizer(line,'g') as gmail,
    ${APP}.base_analizer(line,'hw') as height_width,
    ${APP}.base_analizer(line,'t') as app_time,
    ${APP}.base_analizer(line,'nw') as network,
    ${APP}.base_analizer(line,'ln') as lng,
    ${APP}.base_analizer(line,'la') as lat,
    event_name,
    event_json,
    ${APP}.base_analizer(line,'st') as server_time
from ${APP}.ods_event_log lateral view ${APP}.flat_analizer(${APP}.base_analizer(line,'et')) tem_flat as event_name,event_json
where dt='$do_date'  and ${APP}.base_analizer(line,'et')<>'';
"

$hive -e "$sql";

```

### 临时表拆分到业务事件表

#### 字段说明

商品曝光表

统计商品出现的次数

![1630113073094](大数据学习.assets/1630113073094.png)

```mysql
hive (gmall)> 
drop table if exists dwd_display_log;
CREATE EXTERNAL TABLE dwd_display_log(
`mid_id` string,
`user_id` string,
`version_code` string,
`version_name` string,
`lang` string,
`source` string,
`os` string,
`area` string,
`model` string,
`brand` string,
`sdk_version` string,
`gmail` string,
`height_width` string,
`app_time` string,
`network` string,
`lng` string,
`lat` string,
    
`action` string,
`goodsid` string,
`place` string,
`extend1` string,
`category` string,
    
`server_time` string
)
PARTITIONED BY (dt string)
stored as parquet
location '/warehouse/gmall/dwd/dwd_display_log/'
TBLPROPERTIES('parquet.compression'='lzo');

```

#### 导入数据命令

```mysql
hive (gmall)> 
#动态分区模式：为非严格模式，这里用不到
set hive.exec.dynamic.partition.mode=nonstrict;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
insert overwrite table dwd_display_log
PARTITION (dt='2021-08-11')
select 
mid_id,
user_id,
version_code,
version_name,
lang,
source,
os,
area,
model,
brand,
sdk_version,
gmail,
height_width,
app_time,
network,
lng,
lat,
get_json_object(event_json,'$.kv.action') action,
get_json_object(event_json,'$.kv.goodsid') goodsid,
get_json_object(event_json,'$.kv.place') place,
get_json_object(event_json,'$.kv.extend1') extend1,
get_json_object(event_json,'$.kv.category') category,
server_time
from dwd_base_event_log 
where dt='2021-08-11' and event_name='display';
```



#### 导入命令脚本

```shell
#!/bin/bash

# 定义变量方便修改
APP=gmall
hive=/opt/module/hive/bin/hive

# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
	do_date=$1
else 
	do_date=`date -d "-1 day" +%F`  
fi 

sql="
set hive.exec.dynamic.partition.mode=nonstrict;

insert overwrite table "$APP".dwd_display_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	app_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.action') action,
	get_json_object(event_json,'$.kv.goodsid') goodsid,
	get_json_object(event_json,'$.kv.place') place,
	get_json_object(event_json,'$.kv.extend1') extend1,
	get_json_object(event_json,'$.kv.category') category,
	server_time
from "$APP".dwd_base_event_log 
where dt='$do_date' and event_name='display';


insert overwrite table "$APP".dwd_newsdetail_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	app_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.entry') entry,
	get_json_object(event_json,'$.kv.action') action,
	get_json_object(event_json,'$.kv.goodsid') goodsid,
	get_json_object(event_json,'$.kv.showtype') showtype,
	get_json_object(event_json,'$.kv.news_staytime') news_staytime,
	get_json_object(event_json,'$.kv.loading_time') loading_time,
	get_json_object(event_json,'$.kv.type1') type1,
	get_json_object(event_json,'$.kv.category') category,
	server_time
from "$APP".dwd_base_event_log 
where dt='$do_date' and event_name='newsdetail';


insert overwrite table "$APP".dwd_loading_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	app_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.action') action,
	get_json_object(event_json,'$.kv.loading_time') loading_time,
	get_json_object(event_json,'$.kv.loading_way') loading_way,
	get_json_object(event_json,'$.kv.extend1') extend1,
	get_json_object(event_json,'$.kv.extend2') extend2,
	get_json_object(event_json,'$.kv.type') type,
	get_json_object(event_json,'$.kv.type1') type1,
	server_time
from "$APP".dwd_base_event_log 
where dt='$do_date' and event_name='loading';


insert overwrite table "$APP".dwd_ad_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	app_time,
	network,
	lng,
	lat,
    get_json_object(event_json,'$.kv.entry') entry,
    get_json_object(event_json,'$.kv.action') action,
    get_json_object(event_json,'$.kv.contentType') contentType,
    get_json_object(event_json,'$.kv.displayMills') displayMills,
    get_json_object(event_json,'$.kv.itemId') itemId,
    get_json_object(event_json,'$.kv.activityId') activityId,
	server_time
from "$APP".dwd_base_event_log 
where dt='$do_date' and event_name='ad';


insert overwrite table "$APP".dwd_notification_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	app_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.action') action,
	get_json_object(event_json,'$.kv.noti_type') noti_type,
	get_json_object(event_json,'$.kv.ap_time') ap_time,
	get_json_object(event_json,'$.kv.content') content,
	server_time
from "$APP".dwd_base_event_log 
where dt='$do_date' and event_name='notification';


insert overwrite table "$APP".dwd_active_background_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	app_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.active_source') active_source,
	server_time
from "$APP".dwd_base_event_log 
where dt='$do_date' and event_name='active_background';


insert overwrite table "$APP".dwd_comment_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	app_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.comment_id') comment_id,
	get_json_object(event_json,'$.kv.userid') userid,
	get_json_object(event_json,'$.kv.p_comment_id') p_comment_id,
	get_json_object(event_json,'$.kv.content') content,
	get_json_object(event_json,'$.kv.addtime') addtime,
	get_json_object(event_json,'$.kv.other_id') other_id,
	get_json_object(event_json,'$.kv.praise_count') praise_count,
	get_json_object(event_json,'$.kv.reply_count') reply_count,
	server_time
from "$APP".dwd_base_event_log 
where dt='$do_date' and event_name='comment';


insert overwrite table "$APP".dwd_favorites_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	app_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.id') id,
	get_json_object(event_json,'$.kv.course_id') course_id,
	get_json_object(event_json,'$.kv.userid') userid,
	get_json_object(event_json,'$.kv.add_time') add_time,
	server_time
from "$APP".dwd_base_event_log 
where dt='$do_date' and event_name='favorites';


insert overwrite table "$APP".dwd_praise_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	app_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.id') id,
	get_json_object(event_json,'$.kv.userid') userid,
	get_json_object(event_json,'$.kv.target_id') target_id,
	get_json_object(event_json,'$.kv.type') type,
	get_json_object(event_json,'$.kv.add_time') add_time,
	server_time
from "$APP".dwd_base_event_log 
where dt='$do_date' and event_name='praise';


insert overwrite table "$APP".dwd_error_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	app_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.errorBrief') errorBrief,
	get_json_object(event_json,'$.kv.errorDetail') errorDetail,
	server_time
from "$APP".dwd_base_event_log 
where dt='$do_date' and event_name='error';
"

$hive -e "$sql"

```

yarn调试

```shell
yarn application -list

yarn application -kill 应用id


```

## 4.3、DWD层(业务数据)

![1630121226906](大数据学习.assets/1630121226906.png)

四部曲：

选择业务过程：确定哪些事实表，每个业务过程一个事实表

确定粒度：最小粒度

确定维度：

确定事实：度量字段

最终确定了有哪些事实表，事实表有哪些字段，确定了有哪些维度表

每个维度表是对一个事物的描述，维度表内的字段通过维度退化确定，即把相关的多个表join成一个表。即先子查询过滤出分区数据，再join，分区过滤优先，否则也可以用where过滤(但是会全表查询)

维度表一般采用全量表，表内的数据比较固定

特殊的如用户维度表，每天同步新增及变化，做成拉链表；如客观维度，如地区、只同步一次即可；时间表，一次性同步几年的日历即可。

插入数据都要 set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

### 商品维度表

![1630120928470](大数据学习.assets/1630120928470.png)

```mysql
#用来表征商品的所有信息
dwd_dim_sku_info

drop table if exists dwd_dim_sku_info;
create external table dwd_dim_sku_info(
    `id` string COMMENT '商品id',
    `spu_id` string COMMENT 'spuid',
    `price` double COMMENT '商品价格',
    `sku_name` string COMMENT '商品名称',
    `sku_desc` string COMMENT '商品描述',
    `weight` double COMMENT '重量',
    `tm_id` string COMMENT '品牌id',
    `tm_name` string COMMENT '品牌名称',
    `category3_id` string COMMENT '三级分类id',
    `category2_id` string COMMENT '二级分类id',
    `category1_id` string COMMENT '一级分类id',
    `category3_name` string COMMENT '三级分类名称',
    `category2_name` string COMMENT '二级分类名称',
    `category1_name` string COMMENT '一级分类名称',
    `spu_name` string COMMENT 'spu名称',
    `create_time` string COMMENT '创建时间'
)
comment '商品维度表'
partitioned by (dt string)
stored as parquet
location '/warehouse/gmall/dwd/dwd_dim_sku_info/'
tblproperties("parquet.compression"="lzo");
```

```mysql
#装载语句
use gmall;
hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
insert overwrite into table dwd_dim_sku_info partition(dt='2021-08-11')
select
	sku.id,
	sku.spu_id,
	sku.price,
	sku.sku_name,
	sku.sku_desc,
	sku.weight,
	sku.tm_id,
	tm.tm_name,
	c3.id ,
	c2.id,
	c1.id,
	c3.name,
	c2.name,
	c1.name,
	spu.spu_name,
	sku.create_time
from (select * from ods_sku_info) sku
join (select tm_name from ods_base_trademark) tm
on sku.tm_id=tm.tm_id
join (select * from ods_base_category3) c3
on sku.category3_id=c3.id
join (select * from ods_base_category2) c2
on c3.category2_id=c2.id
join (select * from ods_base_category1) c1
on c2.category1_id=c1.id
join (select * from odw_spu_info) spu
on sku.spu_id=spu.id
```



### 时间维度表

特殊表，一次同步一年的数据，使用时通过dt和日期join即可

通过mysql存储过程往里存；或者java通过jdbc插入数据，在通过mysql导出为txt；或者java直接写到txt

法定节假日手动写

上传data_info文件，文件是txt，但数据表是列式存储，所以需要一个中间表做转换(普通行式存储表)

先作为行式存储导入到临时表中，在插入到列式存储表中，内部会自动转换

```mysql
hive (gmall)> 
DROP TABLE IF EXISTS `dwd_dim_date_info`;
CREATE EXTERNAL TABLE `dwd_dim_date_info`(
    `date_id` string COMMENT '日',
    `week_id` int COMMENT '周',
    `week_day` int COMMENT '周的第几天',
    `day` int COMMENT '每月的第几天',
    `month` int COMMENT '第几月',
    `quarter` int COMMENT '第几季度',
    `year` int COMMENT '年',
    `is_workday` int COMMENT '是否是周末',
    `holiday_id` int COMMENT '是否是节假日'
)
row format delimited fields terminated by '\t'
stored as parquet
location '/warehouse/gmall/dwd/dwd_dim_date_info/'
tblproperties ("parquet.compression"="lzo");

```

### 事实表

#### **分类**

事务型事实表，数据放到表中不会变化，只会新增，不会变化。存储的是当天的新增数据作为分区。想获取所有数据需要join全部分区。增量同步

周期性快照事实表，每日快照，保存的是这个时刻的状态，即周期性的快照，一定的时间周期保存一份。每日全量同步。

累计快照型事实表，累积，用于周期性的业务，比如订单(下单、发货、退款)。新增及变化同步。

#### **事务事实表**

##### 订单明细事实表：

存储的是订单的商品项，度量值是商品数量和订单总金额，度量值为可加的量

##### 订单事实表

累积型，订单状态周期性变化，追踪订单的生命周期

设计事实表时需要考虑好外键，度量值

如果经常使用维度表某个字段，可以在事实表多加这个字段，类似于索引

列式存储，尽量子查询只select需要的内容

##### 退款事实表

时间、用户、商品，度量值：件数、金额。

##### 评价事实表

时间、用户、商品，度量值：个数

#### **周期快照事实表**

##### 购物车事实表

##### 收藏事实表

统计商品收藏的人数，对每个商品count(*)

#### **累积快照事实表**

新增和变化同步

hive下改数据：先select 再改 再insert overwrite

做分区表，先遍历所有表找出需要修改的字段及相应的表，然后只对选出的表进行insert overwrite

##### **优惠卷领用事实表**

**处理原则：**

每日分区仅包含的数据：当日领取优惠卷的记录

而对于：使用优惠卷下单的记录、使用优惠卷支付的记录(应放在领取日期对应的分区，对该分区的该条记录进行修改，即把日期更改为当日)

```mysql
drop table if exists dwd_fact_coupon_use;
create external table dwd_fact_coupon_use(
    `id` string COMMENT '编号',
    `coupon_id` string  COMMENT '优惠券ID',
    `user_id` string  COMMENT 'userid',
    `order_id` string  COMMENT '订单id',
    `coupon_status` string  COMMENT '优惠券状态',
    `get_time` string  COMMENT '领取时间',
    `using_time` string  COMMENT '使用时间(下单)',
    `used_time` string  COMMENT '使用时间(支付)'
) COMMENT '优惠券领用事实表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '\t'
location '/warehouse/gmall/dwd/dwd_fact_coupon_use/';
```

sqoop脚本每日从mysql导入到hdfs的coupon_use的数据为：

![1630142020386](大数据学习.assets/1630142020386.png)

![1630144359281](大数据学习.assets/1630144359281.png)

如果新的有，用新的(加到对应new.get_time的分区里)

新的没有，旧的有，用旧的(加到对应old.get_time的分区里)

```mysql
hive (gmall)> 
#动态分区中，最后一个select值作为动态分区条件
set hive.exec.dynamic.partition.mode=nonstrict;

insert overwrite table dwd_fact_coupon_use partition(dt)
select
	#if类似三元运算符
    if(new.id is null,old.id,new.id),
    if(new.coupon_id is null,old.coupon_id,new.coupon_id),
    if(new.user_id is null,old.user_id,new.user_id),
    if(new.order_id is null,old.order_id,new.order_id),
    if(new.coupon_status is null,old.coupon_status,new.coupon_status),
    if(new.get_time is null,old.get_time,new.get_time),
    if(new.using_time is null,old.using_time,new.using_time),
    if(new.used_time is null,old.used_time,new.used_time),
    #这个字段是为了指明动态分区
    date_format(if(new.get_time is null,old.get_time,new.get_time),'yyyy-MM-dd')
from
(
    select
        id,
        coupon_id,
        user_id,
        order_id,
        coupon_status,
        get_time,
        using_time,
        used_time
    from dwd_fact_coupon_use
    #找出当日ODS层分区记录内，get_time字段对应的所有时间。因为ods层当日分区数据包含三种：当日新增、当日下单、当日支付，其中当日新增的记录中get_time与dt字段时间一致，剩下两种当日下单和当日支付的记录中，get_time与dt字段不一致。
    #即包含三种：当日新增：get_time=dt
    #当日下单：get_time<dt
    #当日支付：get_time<dt
    where dt in
    (
        #从ods层找出所有该日期分区数据
        select
            date_format(get_time,'yyyy-MM-dd')
        from ods_coupon_use
        where dt='2020-03-10'
    )
)old
full outer join
(
    select
        id,
        coupon_id,
        user_id,
        order_id,
        coupon_status,
        get_time,
        using_time,
        used_time
    from ods_coupon_use
    where dt='2020-03-10'
)new
on old.id=new.id;

```

##### 订单事实表

按订单创建时间分区，即下单时间

**时间**分为两种：

create_time 下单时间

opreate_time 订单状态更新时刻 

**状态**分为六种：

![1630156237171](大数据学习.assets/1630156237171.png)

每日采集的业务数据为当日新增订单或者状态发生变化的订单，通过sqoop传输到hdfs，再通过hive加载到ods层

###### 建表

```mysql
hive (gmall)>
drop table if exists dwd_fact_order_info;
create external table dwd_fact_order_info (
    `id` string COMMENT '订单编号',
    `order_status` string COMMENT '订单状态',
    `user_id` string COMMENT '用户id',
    `out_trade_no` string COMMENT '支付流水号',
    `create_time` string COMMENT '创建时间(未支付状态)',
    `payment_time` string COMMENT '支付时间(已支付状态)',
    `cancel_time` string COMMENT '取消时间(已取消状态)',
    `finish_time` string COMMENT '完成时间(已完成状态)',
    `refund_time` string COMMENT '退款时间(退款中状态)',
    `refund_finish_time` string COMMENT '退款完成时间(退款完成状态)',
    `province_id` string COMMENT '省份ID',
    `activity_id` string COMMENT '活动ID',
    `original_total_amount` string COMMENT '原价金额',
    `benefit_reduce_amount` string COMMENT '优惠金额',
    `feight_fee` string COMMENT '运费',
    `final_total_amount` decimal(10,2) COMMENT '订单金额'
) 
PARTITIONED BY (`dt` string)
stored as parquet
location '/warehouse/gmall/dwd/dwd_fact_order_info/'
tblproperties ("parquet.compression"="lzo");

```

###### 导入数据

```mysql
hive (gmall)>
set hive.exec.dynamic.partition.mode=nonstrict;
insert overwrite table dwd_fact_order_info partition(dt)
select
    if(new.id is null,old.id,new.id),
    if(new.order_status is null,old.order_status,new.order_status),
    if(new.user_id is null,old.user_id,new.user_id),
    if(new.out_trade_no is null,old.out_trade_no,new.out_trade_no),
    if(new.tms['1001'] is null,old.create_time,new.tms['1001']),--1001对应未支付状态
    if(new.tms['1002'] is null,old.payment_time,new.tms['1002']),
    if(new.tms['1003'] is null,old.cancel_time,new.tms['1003']),
    if(new.tms['1004'] is null,old.finish_time,new.tms['1004']),
    if(new.tms['1005'] is null,old.refund_time,new.tms['1005']),
    if(new.tms['1006'] is null,old.refund_finish_time,new.tms['1006']),
    if(new.province_id is null,old.province_id,new.province_id),
    if(new.activity_id is null,old.activity_id,new.activity_id),
    if(new.original_total_amount is null,old.original_total_amount,new.original_total_amount),
    if(new.benefit_reduce_amount is null,old.benefit_reduce_amount,new.benefit_reduce_amount),
    if(new.feight_fee is null,old.feight_fee,new.feight_fee),
    if(new.final_total_amount is null,old.final_total_amount,new.final_total_amount),
    date_format(if(new.tms['1001'] is null,old.create_time,new.tms['1001']),'yyyy-MM-dd')
from
#所有要修改的分区数据
(
    select
        id,
        order_status,
        user_id,
        out_trade_no,
        create_time,
        payment_time,
        cancel_time,
        finish_time,
        refund_time,
        refund_finish_time,
        province_id,
        activity_id,
        original_total_amount,
        benefit_reduce_amount,
        feight_fee,
        final_total_amount
    from dwd_fact_order_info
    where dt in
    (
        select
        date_format(create_time,'yyyy-MM-dd')
        from ods_order_info
        where dt='2020-03-10'
    )
)old
full outer join
#由于ODS层表和该事实表字段不一致(六个状态时间)，不能简单地直接select
#ods_order_status_log订单流水表，存储所有订单的所有状态
(
    select
        info.id,
        info.order_status,
        info.user_id,
        info.out_trade_no,
        info.province_id,
        act.activity_id,
        log.tms,
        info.original_total_amount,
        info.benefit_reduce_amount,
        info.feight_fee,
        info.final_total_amount
    from
    (
        select
            order_id,
            str_to_map(concat_ws(',',collect_set(concat(order_status,'=',operate_time))),',','=') tms
        from ods_order_status_log
        where dt='2020-03-10'
        group by order_id
    )log
    join
    (
        select * from ods_order_info where dt='2020-03-10'
    )info
    on log.order_id=info.id
    left join
    (
        select * from ods_activity_order where dt='2020-03-10'
    )act
    on log.order_id=act.order_id
)new
on old.id=new.id;

```

如何把订单日志表转化为每个订单号一行，不同状态为多个列的数据形式

0、先按照order_id分组，把同订单的数据分为一组

1、把其中每一行的数据拼接为kv对，key为order_status，value为operate_time

concat(order_status,'=',operate_time) 

2、通过collect_set聚合函数将每组的多行内容拼接为数组

collect_set(concat(order_status,'=',operate_time))

3、对数组内容用逗号间隔

concat_ws(',',collect_set(concat(order_status,'=',operate_time)))

4、转换为str_to_map,分割为不同的key和value

![1630162900020](大数据学习.assets/1630162900020.png)

![1630157259817](大数据学习.assets/1630157259817.png)

#### 拉链表

每一行属于用户的一个状态，

# 问题处理

## 1、内存报错

hadoop

Container [pid=18309,containerID=container_1628332416938_0001_01_000035] is running 301464064B beyond the 'VIRTUAL' memory limit

 在`/opt/module/hadoop-3.1.4/etc/hadoop/`下的`mapred-site.xml`中添加如下配置 

```xml
<property>
　　<name>mapreduce.map.memory.mb</name>
　　<value>1536</value>
</property>
<property>
　　<name>mapreduce.map.java.opts</name>
　　<value>-Xmx1024M</value>
</property>
<property>
　　<name>mapreduce.reduce.memory.mb</name>
　　<value>2048</value>
</property>
<property>
　　<name>mapreduce.reduce.java.opts</name>
　　<value>-Xmx2048M</value>
</property>
```

## 2、小文件的危害？ 

75个小文件

占用namenode内存  75*150字节  har归档/

增加切片个数 75个maptask

优化方式

1、har归档 对多个小文件进行压缩 这个文件在namenode的占用空间也是150字节

2、自定义inputformat

3、JVM 提高读小文件效率

## 3、在mysql校验证书SSL允许日期之前提交数据报错

比如mysql软件是6月份安装的，插入了4月份的数据，传输就会出错

SSL是安全传输协议

修改连接mysql的命令，加上useSSL=false

```shell
--connect jdbc:mysql://hadoop102:3306/gmall?useSSL=false
```

## 4、hive报错：执行失败

登录mysql，删除hive元数据

重建元数据

hive初始化

启动

此时查询tables会为空，因为虽然数据没有变动，但映射表单的元数据已经被数据化，此时需要重新创建所有数据表，并修复分区

```shell
#修复分区命令
msck repair table 表名称;

#修复分区脚本

#!/bin/bash
case $1 in
"ods_part"){
for i in ods_start_log ods_event_log ods_order_info ods_order_detail ods_sku_info ods_user_info ods_base_category1 ods_base_category2 ods_base_category3 ods_payment_info ods_base_province ods_base_region ods_base_trademark ods_order_status_log ods_spu_info ods_comment_info ods_order_refund_info ods_cart_info ods_favor_info ods_coupon_use ods_coupon_info ods_activity_info ods_activity_order ods_activity_rule ods_base_dic 
do
hive -e "use gmall;msck repair table $i;"
done
};;
esac

```

## 5、hive on Spark无响应

```shell
cd /opt/module/hive/conf


#1、调高spark响应允许时间，重新启动任务看是否能启动
vim hive-site.xml

<property>
	<name>hive.spark.client.connect.timeout</name>
	<value>10000ms</value>
</property>

#2、确定问题位置
登录http://hadoop103:8088/cluster查看任务的logs，锁定问题
通过yarn看任务进行状态，并结束失败还没结束的任务
yarn application -list
yarn appliaction -kill 任务id

#3、如果是资源不足，调低spark集群资源需求
vim spark-defaults.conf

spark.driver.memory                             1g
spark.executor.memory                           1g
```

## 6、服务器网卡无法启动

```shell
ifconfig ens33 up

service NetworkManager stop

service network restart
```

## 7、but there is no HDFS_NAMENODE_USER defined. Aborting operation

edit ***`$HADOOP_HOME/etc/hadoop/hadoop-env.sh`*** by adding the following lines

```javascript
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

Now save and start yarn, [hdfs](https://so.csdn.net/so/search?q=hdfs&spm=1001.2101.3001.7020) service and check that it works.

# 8、jps出现process information unavailable

 进入本地文件系统的**/tmp**目录下，删除名称为**hsperfdata_{username}**的文件夹，然后重新启动Hadoop。 